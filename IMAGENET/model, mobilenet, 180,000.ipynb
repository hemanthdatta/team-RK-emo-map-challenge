{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-10T19:42:27.405218Z","iopub.status.busy":"2024-09-10T19:42:27.404469Z","iopub.status.idle":"2024-09-10T19:42:27.414133Z","shell.execute_reply":"2024-09-10T19:42:27.412716Z","shell.execute_reply.started":"2024-09-10T19:42:27.405168Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:13:20.318726Z","iopub.status.busy":"2024-09-10T20:13:20.318160Z","iopub.status.idle":"2024-09-10T20:13:21.109761Z","shell.execute_reply":"2024-09-10T20:13:21.108466Z","shell.execute_reply.started":"2024-09-10T20:13:20.318670Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>pixels</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>4996</td>\n","      <td>22 24 23 23 25 24 23 20 18 13 6 2 0 1 7 22 32 ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>4997</td>\n","      <td>73 85 87 87 74 118 120 132 134 127 133 118 105...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>4998</td>\n","      <td>253 253 254 254 254 254 250 219 166 141 109 70...</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>4999</td>\n","      <td>78 84 77 95 90 85 72 75 79 84 86 82 88 102 110...</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>5000</td>\n","      <td>98 100 102 104 107 109 111 119 126 130 53 5 12...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 3 columns</p>\n","</div>"],"text/plain":["        id                                             pixels  emotion\n","0        1  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...        0\n","1        2  151 150 147 155 148 133 111 140 170 174 182 15...        0\n","2        3  231 212 156 164 174 138 161 173 182 200 106 38...        2\n","3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...        4\n","4        5  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...        6\n","...    ...                                                ...      ...\n","4995  4996  22 24 23 23 25 24 23 20 18 13 6 2 0 1 7 22 32 ...        3\n","4996  4997  73 85 87 87 74 118 120 132 134 127 133 118 105...        3\n","4997  4998  253 253 254 254 254 254 250 219 166 141 109 70...        6\n","4998  4999  78 84 77 95 90 85 72 75 79 84 86 82 88 102 110...        6\n","4999  5000  98 100 102 104 107 109 111 119 126 130 53 5 12...        3\n","\n","[5000 rows x 3 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train=pd.read_csv('./train_dataset.csv')\n","test=pd.read_csv('./test_dataset.csv')\n","train"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:42:28.781803Z","iopub.status.busy":"2024-09-10T19:42:28.781422Z","iopub.status.idle":"2024-09-10T19:42:28.807307Z","shell.execute_reply":"2024-09-10T19:42:28.806157Z","shell.execute_reply.started":"2024-09-10T19:42:28.781762Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5000 entries, 0 to 4999\n","Data columns (total 3 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   id       5000 non-null   int64 \n"," 1   pixels   5000 non-null   object\n"," 2   emotion  5000 non-null   int64 \n","dtypes: int64(2), object(1)\n","memory usage: 117.3+ KB\n"]}],"source":["train.info()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:59:33.320146Z","iopub.status.busy":"2024-09-10T19:59:33.319617Z","iopub.status.idle":"2024-09-10T19:59:33.331440Z","shell.execute_reply":"2024-09-10T19:59:33.329950Z","shell.execute_reply.started":"2024-09-10T19:59:33.320097Z"},"trusted":true},"outputs":[],"source":["y=train['emotion'].values\n","X=train.drop(['emotion','id'],axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:59:37.489723Z","iopub.status.busy":"2024-09-10T19:59:37.489183Z","iopub.status.idle":"2024-09-10T19:59:41.248449Z","shell.execute_reply":"2024-09-10T19:59:41.247325Z","shell.execute_reply.started":"2024-09-10T19:59:37.489674Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel_1</th>\n","      <th>pixel_2</th>\n","      <th>pixel_3</th>\n","      <th>pixel_4</th>\n","      <th>pixel_5</th>\n","      <th>pixel_6</th>\n","      <th>pixel_7</th>\n","      <th>pixel_8</th>\n","      <th>pixel_9</th>\n","      <th>pixel_10</th>\n","      <th>...</th>\n","      <th>pixel_2295</th>\n","      <th>pixel_2296</th>\n","      <th>pixel_2297</th>\n","      <th>pixel_2298</th>\n","      <th>pixel_2299</th>\n","      <th>pixel_2300</th>\n","      <th>pixel_2301</th>\n","      <th>pixel_2302</th>\n","      <th>pixel_2303</th>\n","      <th>pixel_2304</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>70</td>\n","      <td>80</td>\n","      <td>82</td>\n","      <td>72</td>\n","      <td>58</td>\n","      <td>58</td>\n","      <td>60</td>\n","      <td>63</td>\n","      <td>54</td>\n","      <td>58</td>\n","      <td>...</td>\n","      <td>159</td>\n","      <td>182</td>\n","      <td>183</td>\n","      <td>136</td>\n","      <td>106</td>\n","      <td>116</td>\n","      <td>95</td>\n","      <td>106</td>\n","      <td>109</td>\n","      <td>82</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>151</td>\n","      <td>150</td>\n","      <td>147</td>\n","      <td>155</td>\n","      <td>148</td>\n","      <td>133</td>\n","      <td>111</td>\n","      <td>140</td>\n","      <td>170</td>\n","      <td>174</td>\n","      <td>...</td>\n","      <td>105</td>\n","      <td>108</td>\n","      <td>95</td>\n","      <td>108</td>\n","      <td>102</td>\n","      <td>67</td>\n","      <td>171</td>\n","      <td>193</td>\n","      <td>183</td>\n","      <td>184</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>231</td>\n","      <td>212</td>\n","      <td>156</td>\n","      <td>164</td>\n","      <td>174</td>\n","      <td>138</td>\n","      <td>161</td>\n","      <td>173</td>\n","      <td>182</td>\n","      <td>200</td>\n","      <td>...</td>\n","      <td>104</td>\n","      <td>138</td>\n","      <td>152</td>\n","      <td>122</td>\n","      <td>114</td>\n","      <td>101</td>\n","      <td>97</td>\n","      <td>88</td>\n","      <td>110</td>\n","      <td>152</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>24</td>\n","      <td>32</td>\n","      <td>36</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>23</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>30</td>\n","      <td>41</td>\n","      <td>...</td>\n","      <td>174</td>\n","      <td>126</td>\n","      <td>132</td>\n","      <td>132</td>\n","      <td>133</td>\n","      <td>136</td>\n","      <td>139</td>\n","      <td>142</td>\n","      <td>143</td>\n","      <td>142</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>34</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>27</td>\n","      <td>31</td>\n","      <td>30</td>\n","      <td>29</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>22</td>\n","      <td>24</td>\n","      <td>23</td>\n","      <td>23</td>\n","      <td>25</td>\n","      <td>24</td>\n","      <td>23</td>\n","      <td>20</td>\n","      <td>18</td>\n","      <td>13</td>\n","      <td>...</td>\n","      <td>50</td>\n","      <td>36</td>\n","      <td>36</td>\n","      <td>36</td>\n","      <td>36</td>\n","      <td>36</td>\n","      <td>37</td>\n","      <td>37</td>\n","      <td>36</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>73</td>\n","      <td>85</td>\n","      <td>87</td>\n","      <td>87</td>\n","      <td>74</td>\n","      <td>118</td>\n","      <td>120</td>\n","      <td>132</td>\n","      <td>134</td>\n","      <td>127</td>\n","      <td>...</td>\n","      <td>111</td>\n","      <td>107</td>\n","      <td>70</td>\n","      <td>32</td>\n","      <td>43</td>\n","      <td>89</td>\n","      <td>111</td>\n","      <td>122</td>\n","      <td>78</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>253</td>\n","      <td>253</td>\n","      <td>254</td>\n","      <td>254</td>\n","      <td>254</td>\n","      <td>254</td>\n","      <td>250</td>\n","      <td>219</td>\n","      <td>166</td>\n","      <td>141</td>\n","      <td>...</td>\n","      <td>210</td>\n","      <td>215</td>\n","      <td>220</td>\n","      <td>220</td>\n","      <td>219</td>\n","      <td>220</td>\n","      <td>220</td>\n","      <td>223</td>\n","      <td>222</td>\n","      <td>219</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>78</td>\n","      <td>84</td>\n","      <td>77</td>\n","      <td>95</td>\n","      <td>90</td>\n","      <td>85</td>\n","      <td>72</td>\n","      <td>75</td>\n","      <td>79</td>\n","      <td>84</td>\n","      <td>...</td>\n","      <td>92</td>\n","      <td>89</td>\n","      <td>99</td>\n","      <td>59</td>\n","      <td>17</td>\n","      <td>26</td>\n","      <td>27</td>\n","      <td>34</td>\n","      <td>41</td>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>98</td>\n","      <td>100</td>\n","      <td>102</td>\n","      <td>104</td>\n","      <td>107</td>\n","      <td>109</td>\n","      <td>111</td>\n","      <td>119</td>\n","      <td>126</td>\n","      <td>130</td>\n","      <td>...</td>\n","      <td>69</td>\n","      <td>87</td>\n","      <td>94</td>\n","      <td>100</td>\n","      <td>95</td>\n","      <td>74</td>\n","      <td>123</td>\n","      <td>162</td>\n","      <td>168</td>\n","      <td>195</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 2304 columns</p>\n","</div>"],"text/plain":["     pixel_1 pixel_2 pixel_3 pixel_4 pixel_5 pixel_6 pixel_7 pixel_8 pixel_9  \\\n","0         70      80      82      72      58      58      60      63      54   \n","1        151     150     147     155     148     133     111     140     170   \n","2        231     212     156     164     174     138     161     173     182   \n","3         24      32      36      30      32      23      19      20      30   \n","4          4       0       0       0       0       0       0       0       0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","4995      22      24      23      23      25      24      23      20      18   \n","4996      73      85      87      87      74     118     120     132     134   \n","4997     253     253     254     254     254     254     250     219     166   \n","4998      78      84      77      95      90      85      72      75      79   \n","4999      98     100     102     104     107     109     111     119     126   \n","\n","     pixel_10  ... pixel_2295 pixel_2296 pixel_2297 pixel_2298 pixel_2299  \\\n","0          58  ...        159        182        183        136        106   \n","1         174  ...        105        108         95        108        102   \n","2         200  ...        104        138        152        122        114   \n","3          41  ...        174        126        132        132        133   \n","4           0  ...         12         34         31         31         31   \n","...       ...  ...        ...        ...        ...        ...        ...   \n","4995       13  ...         50         36         36         36         36   \n","4996      127  ...        111        107         70         32         43   \n","4997      141  ...        210        215        220        220        219   \n","4998       84  ...         92         89         99         59         17   \n","4999      130  ...         69         87         94        100         95   \n","\n","     pixel_2300 pixel_2301 pixel_2302 pixel_2303 pixel_2304  \n","0           116         95        106        109         82  \n","1            67        171        193        183        184  \n","2           101         97         88        110        152  \n","3           136        139        142        143        142  \n","4            27         31         30         29         30  \n","...         ...        ...        ...        ...        ...  \n","4995         36         37         37         36         36  \n","4996         89        111        122         78         52  \n","4997        220        220        223        222        219  \n","4998         26         27         34         41         47  \n","4999         74        123        162        168        195  \n","\n","[5000 rows x 2304 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Split the 'pixels' column into multiple columns\n","pixels_split = X['pixels'].str.split(' ', expand=True)\n","\n","# Rename columns to pixel_1, pixel_2, ..., pixel_n\n","pixels_split.columns = [f'pixel_{i+1}' for i in range(pixels_split.shape[1])]\n","\n","# Concatenate the new pixel columns with the original dataframe\n","X= pd.concat([X, pixels_split], axis=1)\n","\n","# Optional: Drop the original 'pixels' column\n","X.drop(columns=['pixels'], inplace=True)\n","X\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:59:45.146716Z","iopub.status.busy":"2024-09-10T19:59:45.146151Z","iopub.status.idle":"2024-09-10T19:59:54.636715Z","shell.execute_reply":"2024-09-10T19:59:54.635580Z","shell.execute_reply.started":"2024-09-10T19:59:45.146670Z"},"trusted":true},"outputs":[],"source":["X=X.apply(pd.to_numeric)\n","X=X.values"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:13:32.416904Z","iopub.status.busy":"2024-09-10T20:13:32.416360Z","iopub.status.idle":"2024-09-10T20:13:34.355894Z","shell.execute_reply":"2024-09-10T20:13:34.354573Z","shell.execute_reply.started":"2024-09-10T20:13:32.416858Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel_1</th>\n","      <th>pixel_2</th>\n","      <th>pixel_3</th>\n","      <th>pixel_4</th>\n","      <th>pixel_5</th>\n","      <th>pixel_6</th>\n","      <th>pixel_7</th>\n","      <th>pixel_8</th>\n","      <th>pixel_9</th>\n","      <th>pixel_10</th>\n","      <th>...</th>\n","      <th>pixel_2295</th>\n","      <th>pixel_2296</th>\n","      <th>pixel_2297</th>\n","      <th>pixel_2298</th>\n","      <th>pixel_2299</th>\n","      <th>pixel_2300</th>\n","      <th>pixel_2301</th>\n","      <th>pixel_2302</th>\n","      <th>pixel_2303</th>\n","      <th>pixel_2304</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>80</td>\n","      <td>81</td>\n","      <td>77</td>\n","      <td>69</td>\n","      <td>66</td>\n","      <td>59</td>\n","      <td>70</td>\n","      <td>89</td>\n","      <td>112</td>\n","      <td>132</td>\n","      <td>...</td>\n","      <td>70</td>\n","      <td>68</td>\n","      <td>67</td>\n","      <td>56</td>\n","      <td>64</td>\n","      <td>70</td>\n","      <td>75</td>\n","      <td>84</td>\n","      <td>75</td>\n","      <td>70</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>226</td>\n","      <td>226</td>\n","      <td>226</td>\n","      <td>217</td>\n","      <td>203</td>\n","      <td>189</td>\n","      <td>97</td>\n","      <td>149</td>\n","      <td>193</td>\n","      <td>193</td>\n","      <td>...</td>\n","      <td>48</td>\n","      <td>69</td>\n","      <td>106</td>\n","      <td>114</td>\n","      <td>88</td>\n","      <td>89</td>\n","      <td>36</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>98</td>\n","      <td>112</td>\n","      <td>43</td>\n","      <td>41</td>\n","      <td>46</td>\n","      <td>47</td>\n","      <td>67</td>\n","      <td>37</td>\n","      <td>27</td>\n","      <td>37</td>\n","      <td>...</td>\n","      <td>62</td>\n","      <td>48</td>\n","      <td>29</td>\n","      <td>28</td>\n","      <td>33</td>\n","      <td>35</td>\n","      <td>36</td>\n","      <td>40</td>\n","      <td>40</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>35</td>\n","      <td>38</td>\n","      <td>29</td>\n","      <td>25</td>\n","      <td>21</td>\n","      <td>29</td>\n","      <td>35</td>\n","      <td>32</td>\n","      <td>41</td>\n","      <td>49</td>\n","      <td>...</td>\n","      <td>142</td>\n","      <td>139</td>\n","      <td>171</td>\n","      <td>201</td>\n","      <td>213</td>\n","      <td>219</td>\n","      <td>219</td>\n","      <td>226</td>\n","      <td>252</td>\n","      <td>254</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>19</td>\n","      <td>14</td>\n","      <td>15</td>\n","      <td>21</td>\n","      <td>50</td>\n","      <td>73</td>\n","      <td>73</td>\n","      <td>...</td>\n","      <td>145</td>\n","      <td>151</td>\n","      <td>157</td>\n","      <td>160</td>\n","      <td>157</td>\n","      <td>158</td>\n","      <td>170</td>\n","      <td>185</td>\n","      <td>191</td>\n","      <td>194</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2495</th>\n","      <td>50</td>\n","      <td>36</td>\n","      <td>17</td>\n","      <td>22</td>\n","      <td>23</td>\n","      <td>29</td>\n","      <td>33</td>\n","      <td>39</td>\n","      <td>34</td>\n","      <td>37</td>\n","      <td>...</td>\n","      <td>216</td>\n","      <td>215</td>\n","      <td>216</td>\n","      <td>217</td>\n","      <td>221</td>\n","      <td>222</td>\n","      <td>220</td>\n","      <td>223</td>\n","      <td>221</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>2496</th>\n","      <td>178</td>\n","      <td>174</td>\n","      <td>172</td>\n","      <td>173</td>\n","      <td>181</td>\n","      <td>188</td>\n","      <td>191</td>\n","      <td>194</td>\n","      <td>196</td>\n","      <td>199</td>\n","      <td>...</td>\n","      <td>147</td>\n","      <td>141</td>\n","      <td>136</td>\n","      <td>118</td>\n","      <td>66</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2497</th>\n","      <td>17</td>\n","      <td>17</td>\n","      <td>16</td>\n","      <td>23</td>\n","      <td>28</td>\n","      <td>22</td>\n","      <td>19</td>\n","      <td>17</td>\n","      <td>25</td>\n","      <td>26</td>\n","      <td>...</td>\n","      <td>190</td>\n","      <td>179</td>\n","      <td>193</td>\n","      <td>193</td>\n","      <td>194</td>\n","      <td>170</td>\n","      <td>148</td>\n","      <td>154</td>\n","      <td>133</td>\n","      <td>113</td>\n","    </tr>\n","    <tr>\n","      <th>2498</th>\n","      <td>30</td>\n","      <td>28</td>\n","      <td>28</td>\n","      <td>29</td>\n","      <td>31</td>\n","      <td>30</td>\n","      <td>42</td>\n","      <td>68</td>\n","      <td>79</td>\n","      <td>81</td>\n","      <td>...</td>\n","      <td>30</td>\n","      <td>27</td>\n","      <td>27</td>\n","      <td>26</td>\n","      <td>28</td>\n","      <td>35</td>\n","      <td>35</td>\n","      <td>35</td>\n","      <td>30</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>19</td>\n","      <td>13</td>\n","      <td>14</td>\n","      <td>12</td>\n","      <td>13</td>\n","      <td>16</td>\n","      <td>21</td>\n","      <td>33</td>\n","      <td>50</td>\n","      <td>57</td>\n","      <td>...</td>\n","      <td>224</td>\n","      <td>217</td>\n","      <td>209</td>\n","      <td>195</td>\n","      <td>151</td>\n","      <td>99</td>\n","      <td>146</td>\n","      <td>189</td>\n","      <td>199</td>\n","      <td>201</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2500 rows × 2304 columns</p>\n","</div>"],"text/plain":["     pixel_1 pixel_2 pixel_3 pixel_4 pixel_5 pixel_6 pixel_7 pixel_8 pixel_9  \\\n","0         80      81      77      69      66      59      70      89     112   \n","1        226     226     226     217     203     189      97     149     193   \n","2         98     112      43      41      46      47      67      37      27   \n","3         35      38      29      25      21      29      35      32      41   \n","4          4       1       5      19      14      15      21      50      73   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","2495      50      36      17      22      23      29      33      39      34   \n","2496     178     174     172     173     181     188     191     194     196   \n","2497      17      17      16      23      28      22      19      17      25   \n","2498      30      28      28      29      31      30      42      68      79   \n","2499      19      13      14      12      13      16      21      33      50   \n","\n","     pixel_10  ... pixel_2295 pixel_2296 pixel_2297 pixel_2298 pixel_2299  \\\n","0         132  ...         70         68         67         56         64   \n","1         193  ...         48         69        106        114         88   \n","2          37  ...         62         48         29         28         33   \n","3          49  ...        142        139        171        201        213   \n","4          73  ...        145        151        157        160        157   \n","...       ...  ...        ...        ...        ...        ...        ...   \n","2495       37  ...        216        215        216        217        221   \n","2496      199  ...        147        141        136        118         66   \n","2497       26  ...        190        179        193        193        194   \n","2498       81  ...         30         27         27         26         28   \n","2499       57  ...        224        217        209        195        151   \n","\n","     pixel_2300 pixel_2301 pixel_2302 pixel_2303 pixel_2304  \n","0            70         75         84         75         70  \n","1            89         36         22         26        105  \n","2            35         36         40         40         39  \n","3           219        219        226        252        254  \n","4           158        170        185        191        194  \n","...         ...        ...        ...        ...        ...  \n","2495        222        220        223        221        216  \n","2496          7          0          0          0          0  \n","2497        170        148        154        133        113  \n","2498         35         35         35         30         28  \n","2499         99        146        189        199        201  \n","\n","[2500 rows x 2304 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Split the 'pixels' column into multiple columns\n","pixels_split = test['pixels'].str.split(' ', expand=True)\n","\n","# Rename columns to pixel_1, pixel_2, ..., pixel_n\n","pixels_split.columns = [f'pixel_{i+1}' for i in range(pixels_split.shape[1])]\n","\n","# Concatenate the new pixel columns with the original dataframe\n","test= pd.concat([test, pixels_split], axis=1)\n","id=test.id\n","# Optional: Drop the original 'pixels' column\n","test.drop(columns=['pixels','id'], inplace=True)\n","test\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:42:35.022097Z","iopub.status.busy":"2024-09-10T19:42:35.021745Z","iopub.status.idle":"2024-09-10T19:42:35.030942Z","shell.execute_reply":"2024-09-10T19:42:35.029704Z","shell.execute_reply.started":"2024-09-10T19:42:35.022048Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(5000, 2304)\n"]},{"data":{"text/plain":["(2500, 2304)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["print(X.shape)\n","test.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T19:51:04.278652Z","iopub.status.busy":"2024-09-10T19:51:04.278041Z","iopub.status.idle":"2024-09-10T19:51:19.602271Z","shell.execute_reply":"2024-09-10T19:51:19.600901Z","shell.execute_reply.started":"2024-09-10T19:51:04.278601Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:00:22.183045Z","iopub.status.busy":"2024-09-10T20:00:22.182471Z","iopub.status.idle":"2024-09-10T20:00:22.219249Z","shell.execute_reply":"2024-09-10T20:00:22.217974Z","shell.execute_reply.started":"2024-09-10T20:00:22.182991Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X: (5000, 48, 48, 1)\n","Shape of y: (5000, 7)\n"]}],"source":["image_size = 48\n","X = X.reshape(-1, image_size, image_size, 1)  # Reshape to 48x48 images with 1 channel (grayscale)\n","# Normalize pixel values to [0, 1]\n","X = X / 255.0\n","\n","# One-hot encode the labels (since they range from 0 to 6, indicating 7 classes)\n","y = to_categorical(y, num_classes=7)\n","\n","# Check the shapes of X and y\n","print(f'Shape of X: {X.shape}')\n","print(f'Shape of y: {y.shape}')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:22:35.205353Z","iopub.status.busy":"2024-09-10T20:22:35.204679Z","iopub.status.idle":"2024-09-10T20:22:35.367276Z","shell.execute_reply":"2024-09-10T20:22:35.365980Z","shell.execute_reply.started":"2024-09-10T20:22:35.205297Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X_train: (4500, 48, 48, 1)\n","Shape of X_test: (500, 48, 48, 1)\n","Shape of y_train: (4500, 7)\n","Shape of y_test: (500, 7)\n"]}],"source":["# Split the dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=32)\n","\n","# Check the shapes of the split data\n","print(f'Shape of X_train: {X_train.shape}')\n","print(f'Shape of X_test: {X_test.shape}')\n","print(f'Shape of y_train: {y_train.shape}')\n","print(f'Shape of y_test: {y_test.shape}')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T05:30:07.704809Z","iopub.status.busy":"2024-09-11T05:30:07.704305Z","iopub.status.idle":"2024-09-11T05:30:24.243283Z","shell.execute_reply":"2024-09-11T05:30:24.241384Z","shell.execute_reply.started":"2024-09-11T05:30:07.704766Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 48, 48, 1)]       0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 48, 48, 1)        4         \n"," ormalization)                                                   \n","                                                                 \n"," conv2d (Conv2D)             (None, 48, 48, 64)        640       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," depthwise_conv2d (Depthwise  (None, 48, 48, 64)       640       \n"," Conv2D)                                                         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 48, 48, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 48, 48, 128)       8320      \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 48, 48, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 24, 24, 128)      0         \n"," )                                                               \n","                                                                 \n"," depthwise_conv2d_1 (Depthwi  (None, 24, 24, 128)      1280      \n"," seConv2D)                                                       \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 24, 24, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 24, 24, 256)       33024     \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 24, 24, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," depthwise_conv2d_2 (Depthwi  (None, 12, 12, 256)      2560      \n"," seConv2D)                                                       \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 12, 12, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 12, 12, 256)       65792     \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 12, 12, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 256)       0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 256)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 256)               65792     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 7)                 1799      \n","                                                                 \n","=================================================================\n","Total params: 184,459\n","Trainable params: 182,153\n","Non-trainable params: 2,306\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, DepthwiseConv2D, ReLU, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D\n","from tensorflow.keras.models import Model\n","\n","# Input layer with fixed 48x48 dimensions and 1 channel\n","inputs = Input(shape=(48, 48, 1))\n","\n","# Initial Batch Normalization\n","x = BatchNormalization()(inputs)\n","\n","# Initial Conv2D layer with doubled filters (64 instead of 32)\n","x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","\n","# Depthwise Separable Convolution Block 1 with more filters\n","x = DepthwiseConv2D(kernel_size=(3, 3), depth_multiplier=1, padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Conv2D(128, (1, 1), padding='same', activation='relu')(x)  # Pointwise convolution (128 instead of 64)\n","x = BatchNormalization()(x)\n","\n","# MaxPooling to downsample spatial dimensions\n","x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","# Depthwise Separable Convolution Block 2 with more filters\n","x = DepthwiseConv2D(kernel_size=(3, 3), depth_multiplier=1, padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Conv2D(256, (1, 1), padding='same', activation='relu')(x)  # Pointwise convolution (256 instead of 128)\n","x = BatchNormalization()(x)\n","\n","# MaxPooling to further downsample\n","x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","# Depthwise Separable Convolution Block 3 with more filters\n","x = DepthwiseConv2D(kernel_size=(3, 3), depth_multiplier=1, padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Conv2D(256, (1, 1), padding='same', activation='relu')(x)  # Pointwise convolution (256 instead of 128)\n","x = BatchNormalization()(x)\n","\n","# Spatial Dropout to prevent overfitting\n","x = Dropout(0.3)(x)\n","\n","# Global Average Pooling and Dropout\n","x = GlobalAveragePooling2D()(x)\n","x = Dropout(0.5)(x)\n","\n","# Fully connected dense layer with more neurons (256 instead of 128)\n","x = Dense(256, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","\n","# Output layer for 7 classes (corresponding to 7 facial expressions)\n","output = Dense(7, activation='softmax')(x)\n","\n","# Create the model\n","model = Model(inputs, output)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Summary of the model\n","model.summary()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import keras\n","checkpoint = keras.callbacks.ModelCheckpoint('ckpt0.5.keras',monitor = 'val_accuracy', verbose = 1, save_best_only=True, mode ='max')\n","\n","stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 300)\n","try:\n"," model.load_weights('ckpt0.5.keras')\n","except:print('creating new')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2024-09-11T05:30:24.244475Z","iopub.status.idle":"2024-09-11T05:30:24.244946Z","shell.execute_reply":"2024-09-11T05:30:24.244743Z","shell.execute_reply.started":"2024-09-11T05:30:24.244720Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 48, 48, 1)]       0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 48, 48, 1)        4         \n"," ormalization)                                                   \n","                                                                 \n"," conv2d (Conv2D)             (None, 48, 48, 64)        640       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," depthwise_conv2d (Depthwise  (None, 48, 48, 64)       640       \n"," Conv2D)                                                         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 48, 48, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 48, 48, 128)       8320      \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 48, 48, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 24, 24, 128)      0         \n"," )                                                               \n","                                                                 \n"," depthwise_conv2d_1 (Depthwi  (None, 24, 24, 128)      1280      \n"," seConv2D)                                                       \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 24, 24, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 24, 24, 256)       33024     \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 24, 24, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," depthwise_conv2d_2 (Depthwi  (None, 12, 12, 256)      2560      \n"," seConv2D)                                                       \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 12, 12, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 12, 12, 256)       65792     \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 12, 12, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 256)       0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 256)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 256)               65792     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 7)                 1799      \n","                                                                 \n","=================================================================\n","Total params: 184,459\n","Trainable params: 182,153\n","Non-trainable params: 2,306\n","_________________________________________________________________\n"]}],"source":["# Compile the model\n","model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print the model summary to see the architecture\n","model.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["X_train=np.multiply(X_train,10)\n","X_test = np.multiply(X_test,10)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/500\n","\n","Epoch 1: val_accuracy improved from -inf to 0.58000, saving model to ckpt0.5.keras\n","141/141 - 10s - loss: 0.7542 - accuracy: 0.7178 - val_loss: 1.4512 - val_accuracy: 0.5800 - 10s/epoch - 67ms/step\n","Epoch 2/500\n","\n","Epoch 2: val_accuracy did not improve from 0.58000\n","141/141 - 2s - loss: 0.7506 - accuracy: 0.7204 - val_loss: 1.4821 - val_accuracy: 0.5640 - 2s/epoch - 18ms/step\n","Epoch 3/500\n","\n","Epoch 3: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7405 - accuracy: 0.7309 - val_loss: 1.5169 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 4/500\n","\n","Epoch 4: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7577 - accuracy: 0.7173 - val_loss: 1.4708 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 5/500\n","\n","Epoch 5: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7435 - accuracy: 0.7282 - val_loss: 1.4373 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 6/500\n","\n","Epoch 6: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7273 - accuracy: 0.7356 - val_loss: 1.4778 - val_accuracy: 0.5700 - 3s/epoch - 18ms/step\n","Epoch 7/500\n","\n","Epoch 7: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7241 - accuracy: 0.7298 - val_loss: 1.4664 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 8/500\n","\n","Epoch 8: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7406 - accuracy: 0.7249 - val_loss: 1.3861 - val_accuracy: 0.5680 - 3s/epoch - 18ms/step\n","Epoch 9/500\n","\n","Epoch 9: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7370 - accuracy: 0.7331 - val_loss: 1.4688 - val_accuracy: 0.5680 - 3s/epoch - 18ms/step\n","Epoch 10/500\n","\n","Epoch 10: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7369 - accuracy: 0.7280 - val_loss: 1.5056 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 11/500\n","\n","Epoch 11: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7281 - accuracy: 0.7258 - val_loss: 1.4577 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 12/500\n","\n","Epoch 12: val_accuracy did not improve from 0.58000\n","141/141 - 2s - loss: 0.7278 - accuracy: 0.7278 - val_loss: 1.4849 - val_accuracy: 0.5600 - 2s/epoch - 18ms/step\n","Epoch 13/500\n","\n","Epoch 13: val_accuracy did not improve from 0.58000\n","141/141 - 3s - loss: 0.7191 - accuracy: 0.7276 - val_loss: 1.4640 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 14/500\n","\n","Epoch 14: val_accuracy improved from 0.58000 to 0.58400, saving model to ckpt0.5.keras\n","141/141 - 3s - loss: 0.7173 - accuracy: 0.7307 - val_loss: 1.4473 - val_accuracy: 0.5840 - 3s/epoch - 19ms/step\n","Epoch 15/500\n","\n","Epoch 15: val_accuracy did not improve from 0.58400\n","141/141 - 3s - loss: 0.7343 - accuracy: 0.7258 - val_loss: 1.5063 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 16/500\n","\n","Epoch 16: val_accuracy did not improve from 0.58400\n","141/141 - 3s - loss: 0.6932 - accuracy: 0.7364 - val_loss: 1.5516 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 17/500\n","\n","Epoch 17: val_accuracy did not improve from 0.58400\n","141/141 - 3s - loss: 0.6941 - accuracy: 0.7433 - val_loss: 1.5164 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 18/500\n","\n","Epoch 18: val_accuracy improved from 0.58400 to 0.59400, saving model to ckpt0.5.keras\n","141/141 - 3s - loss: 0.6850 - accuracy: 0.7487 - val_loss: 1.4322 - val_accuracy: 0.5940 - 3s/epoch - 19ms/step\n","Epoch 19/500\n","\n","Epoch 19: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6960 - accuracy: 0.7360 - val_loss: 1.4747 - val_accuracy: 0.5880 - 3s/epoch - 18ms/step\n","Epoch 20/500\n","\n","Epoch 20: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.7259 - accuracy: 0.7269 - val_loss: 1.4308 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 21/500\n","\n","Epoch 21: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.7190 - accuracy: 0.7340 - val_loss: 1.5114 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 22/500\n","\n","Epoch 22: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.7002 - accuracy: 0.7298 - val_loss: 1.5095 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 23/500\n","\n","Epoch 23: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.7035 - accuracy: 0.7409 - val_loss: 1.4915 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 24/500\n","\n","Epoch 24: val_accuracy did not improve from 0.59400\n","141/141 - 2s - loss: 0.6979 - accuracy: 0.7396 - val_loss: 1.5557 - val_accuracy: 0.5560 - 2s/epoch - 18ms/step\n","Epoch 25/500\n","\n","Epoch 25: val_accuracy did not improve from 0.59400\n","141/141 - 2s - loss: 0.7011 - accuracy: 0.7349 - val_loss: 1.4672 - val_accuracy: 0.5740 - 2s/epoch - 18ms/step\n","Epoch 26/500\n","\n","Epoch 26: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.7194 - accuracy: 0.7387 - val_loss: 1.5118 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 27/500\n","\n","Epoch 27: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6959 - accuracy: 0.7360 - val_loss: 1.4172 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 28/500\n","\n","Epoch 28: val_accuracy did not improve from 0.59400\n","141/141 - 2s - loss: 0.7125 - accuracy: 0.7362 - val_loss: 1.5073 - val_accuracy: 0.5720 - 2s/epoch - 18ms/step\n","Epoch 29/500\n","\n","Epoch 29: val_accuracy did not improve from 0.59400\n","141/141 - 2s - loss: 0.7047 - accuracy: 0.7389 - val_loss: 1.5351 - val_accuracy: 0.5500 - 2s/epoch - 18ms/step\n","Epoch 30/500\n","\n","Epoch 30: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6885 - accuracy: 0.7478 - val_loss: 1.4112 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 31/500\n","\n","Epoch 31: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6945 - accuracy: 0.7396 - val_loss: 1.5657 - val_accuracy: 0.5520 - 3s/epoch - 18ms/step\n","Epoch 32/500\n","\n","Epoch 32: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6932 - accuracy: 0.7416 - val_loss: 1.4087 - val_accuracy: 0.5700 - 3s/epoch - 18ms/step\n","Epoch 33/500\n","\n","Epoch 33: val_accuracy did not improve from 0.59400\n","141/141 - 3s - loss: 0.6912 - accuracy: 0.7436 - val_loss: 1.5854 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 34/500\n","\n","Epoch 34: val_accuracy improved from 0.59400 to 0.60000, saving model to ckpt0.5.keras\n","141/141 - 3s - loss: 0.7007 - accuracy: 0.7418 - val_loss: 1.4242 - val_accuracy: 0.6000 - 3s/epoch - 19ms/step\n","Epoch 35/500\n","\n","Epoch 35: val_accuracy did not improve from 0.60000\n","141/141 - 3s - loss: 0.6967 - accuracy: 0.7382 - val_loss: 1.4257 - val_accuracy: 0.5980 - 3s/epoch - 18ms/step\n","Epoch 36/500\n","\n","Epoch 36: val_accuracy did not improve from 0.60000\n","141/141 - 3s - loss: 0.6829 - accuracy: 0.7444 - val_loss: 1.4382 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 37/500\n","\n","Epoch 37: val_accuracy did not improve from 0.60000\n","141/141 - 3s - loss: 0.7058 - accuracy: 0.7380 - val_loss: 1.3965 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 38/500\n","\n","Epoch 38: val_accuracy improved from 0.60000 to 0.60200, saving model to ckpt0.5.keras\n","141/141 - 3s - loss: 0.6816 - accuracy: 0.7513 - val_loss: 1.4344 - val_accuracy: 0.6020 - 3s/epoch - 19ms/step\n","Epoch 39/500\n","\n","Epoch 39: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6912 - accuracy: 0.7413 - val_loss: 1.5417 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 40/500\n","\n","Epoch 40: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6948 - accuracy: 0.7378 - val_loss: 1.4259 - val_accuracy: 0.5920 - 3s/epoch - 18ms/step\n","Epoch 41/500\n","\n","Epoch 41: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6948 - accuracy: 0.7478 - val_loss: 1.4542 - val_accuracy: 0.5880 - 3s/epoch - 18ms/step\n","Epoch 42/500\n","\n","Epoch 42: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.7034 - accuracy: 0.7456 - val_loss: 1.4991 - val_accuracy: 0.5540 - 3s/epoch - 18ms/step\n","Epoch 43/500\n","\n","Epoch 43: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6695 - accuracy: 0.7591 - val_loss: 1.5441 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 44/500\n","\n","Epoch 44: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6961 - accuracy: 0.7424 - val_loss: 1.4769 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 45/500\n","\n","Epoch 45: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.7003 - accuracy: 0.7362 - val_loss: 1.5405 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 46/500\n","\n","Epoch 46: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.7004 - accuracy: 0.7349 - val_loss: 1.3922 - val_accuracy: 0.6020 - 3s/epoch - 18ms/step\n","Epoch 47/500\n","\n","Epoch 47: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6893 - accuracy: 0.7442 - val_loss: 1.4401 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 48/500\n","\n","Epoch 48: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6789 - accuracy: 0.7527 - val_loss: 1.4653 - val_accuracy: 0.5820 - 3s/epoch - 18ms/step\n","Epoch 49/500\n","\n","Epoch 49: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6983 - accuracy: 0.7418 - val_loss: 1.4700 - val_accuracy: 0.5820 - 3s/epoch - 18ms/step\n","Epoch 50/500\n","\n","Epoch 50: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6695 - accuracy: 0.7451 - val_loss: 1.4666 - val_accuracy: 0.5840 - 3s/epoch - 18ms/step\n","Epoch 51/500\n","\n","Epoch 51: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6712 - accuracy: 0.7524 - val_loss: 1.3911 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 52/500\n","\n","Epoch 52: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6848 - accuracy: 0.7482 - val_loss: 1.4041 - val_accuracy: 0.6000 - 3s/epoch - 18ms/step\n","Epoch 53/500\n","\n","Epoch 53: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6958 - accuracy: 0.7391 - val_loss: 1.4509 - val_accuracy: 0.5820 - 3s/epoch - 18ms/step\n","Epoch 54/500\n","\n","Epoch 54: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.7140 - accuracy: 0.7422 - val_loss: 1.4369 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 55/500\n","\n","Epoch 55: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6808 - accuracy: 0.7498 - val_loss: 1.4347 - val_accuracy: 0.5840 - 2s/epoch - 18ms/step\n","Epoch 56/500\n","\n","Epoch 56: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6986 - accuracy: 0.7456 - val_loss: 1.4102 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 57/500\n","\n","Epoch 57: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6855 - accuracy: 0.7478 - val_loss: 1.4734 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 58/500\n","\n","Epoch 58: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6846 - accuracy: 0.7480 - val_loss: 1.4853 - val_accuracy: 0.5680 - 3s/epoch - 18ms/step\n","Epoch 59/500\n","\n","Epoch 59: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6590 - accuracy: 0.7560 - val_loss: 1.4938 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 60/500\n","\n","Epoch 60: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6680 - accuracy: 0.7491 - val_loss: 1.4450 - val_accuracy: 0.5980 - 3s/epoch - 18ms/step\n","Epoch 61/500\n","\n","Epoch 61: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6683 - accuracy: 0.7571 - val_loss: 1.5050 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 62/500\n","\n","Epoch 62: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6638 - accuracy: 0.7487 - val_loss: 1.5349 - val_accuracy: 0.5820 - 3s/epoch - 18ms/step\n","Epoch 63/500\n","\n","Epoch 63: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6900 - accuracy: 0.7387 - val_loss: 1.4373 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 64/500\n","\n","Epoch 64: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6790 - accuracy: 0.7542 - val_loss: 1.5479 - val_accuracy: 0.5300 - 3s/epoch - 18ms/step\n","Epoch 65/500\n","\n","Epoch 65: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6895 - accuracy: 0.7460 - val_loss: 1.4618 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 66/500\n","\n","Epoch 66: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6801 - accuracy: 0.7513 - val_loss: 1.4898 - val_accuracy: 0.5680 - 3s/epoch - 18ms/step\n","Epoch 67/500\n","\n","Epoch 67: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6957 - accuracy: 0.7382 - val_loss: 1.4619 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 68/500\n","\n","Epoch 68: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6842 - accuracy: 0.7509 - val_loss: 1.3840 - val_accuracy: 0.5920 - 3s/epoch - 18ms/step\n","Epoch 69/500\n","\n","Epoch 69: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6831 - accuracy: 0.7496 - val_loss: 1.4630 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 70/500\n","\n","Epoch 70: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6727 - accuracy: 0.7387 - val_loss: 1.4850 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 71/500\n","\n","Epoch 71: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6817 - accuracy: 0.7458 - val_loss: 1.4459 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 72/500\n","\n","Epoch 72: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6671 - accuracy: 0.7571 - val_loss: 1.5411 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 73/500\n","\n","Epoch 73: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6697 - accuracy: 0.7504 - val_loss: 1.6722 - val_accuracy: 0.5320 - 2s/epoch - 18ms/step\n","Epoch 74/500\n","\n","Epoch 74: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6691 - accuracy: 0.7538 - val_loss: 1.5453 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 75/500\n","\n","Epoch 75: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6737 - accuracy: 0.7473 - val_loss: 1.5457 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 76/500\n","\n","Epoch 76: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6777 - accuracy: 0.7533 - val_loss: 1.4250 - val_accuracy: 0.5940 - 3s/epoch - 18ms/step\n","Epoch 77/500\n","\n","Epoch 77: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6736 - accuracy: 0.7542 - val_loss: 1.5485 - val_accuracy: 0.5540 - 2s/epoch - 18ms/step\n","Epoch 78/500\n","\n","Epoch 78: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6665 - accuracy: 0.7560 - val_loss: 1.5299 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 79/500\n","\n","Epoch 79: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6544 - accuracy: 0.7484 - val_loss: 1.4864 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 80/500\n","\n","Epoch 80: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6609 - accuracy: 0.7529 - val_loss: 1.6270 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 81/500\n","\n","Epoch 81: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6654 - accuracy: 0.7520 - val_loss: 1.5040 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 82/500\n","\n","Epoch 82: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6563 - accuracy: 0.7640 - val_loss: 1.5162 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 83/500\n","\n","Epoch 83: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6587 - accuracy: 0.7520 - val_loss: 1.6916 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 84/500\n","\n","Epoch 84: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6680 - accuracy: 0.7511 - val_loss: 1.5105 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 85/500\n","\n","Epoch 85: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6631 - accuracy: 0.7531 - val_loss: 1.6019 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 86/500\n","\n","Epoch 86: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6638 - accuracy: 0.7633 - val_loss: 1.5643 - val_accuracy: 0.5560 - 3s/epoch - 18ms/step\n","Epoch 87/500\n","\n","Epoch 87: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6681 - accuracy: 0.7547 - val_loss: 1.4913 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 88/500\n","\n","Epoch 88: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6884 - accuracy: 0.7471 - val_loss: 1.4072 - val_accuracy: 0.5540 - 3s/epoch - 18ms/step\n","Epoch 89/500\n","\n","Epoch 89: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6644 - accuracy: 0.7569 - val_loss: 1.5267 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 90/500\n","\n","Epoch 90: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6602 - accuracy: 0.7576 - val_loss: 1.4188 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 91/500\n","\n","Epoch 91: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6507 - accuracy: 0.7580 - val_loss: 1.4588 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 92/500\n","\n","Epoch 92: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6712 - accuracy: 0.7529 - val_loss: 1.5221 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 93/500\n","\n","Epoch 93: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6384 - accuracy: 0.7647 - val_loss: 1.6739 - val_accuracy: 0.5320 - 3s/epoch - 18ms/step\n","Epoch 94/500\n","\n","Epoch 94: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6728 - accuracy: 0.7507 - val_loss: 1.5856 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 95/500\n","\n","Epoch 95: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6795 - accuracy: 0.7511 - val_loss: 1.5027 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 96/500\n","\n","Epoch 96: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6680 - accuracy: 0.7538 - val_loss: 1.5737 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 97/500\n","\n","Epoch 97: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6594 - accuracy: 0.7449 - val_loss: 1.4946 - val_accuracy: 0.5880 - 3s/epoch - 18ms/step\n","Epoch 98/500\n","\n","Epoch 98: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6315 - accuracy: 0.7684 - val_loss: 1.5344 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 99/500\n","\n","Epoch 99: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6483 - accuracy: 0.7496 - val_loss: 1.5910 - val_accuracy: 0.5260 - 3s/epoch - 18ms/step\n","Epoch 100/500\n","\n","Epoch 100: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6432 - accuracy: 0.7604 - val_loss: 1.5943 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 101/500\n","\n","Epoch 101: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6785 - accuracy: 0.7553 - val_loss: 1.4810 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 102/500\n","\n","Epoch 102: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6510 - accuracy: 0.7549 - val_loss: 1.5732 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 103/500\n","\n","Epoch 103: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6736 - accuracy: 0.7542 - val_loss: 1.5243 - val_accuracy: 0.5300 - 3s/epoch - 18ms/step\n","Epoch 104/500\n","\n","Epoch 104: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6683 - accuracy: 0.7504 - val_loss: 1.4788 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 105/500\n","\n","Epoch 105: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6664 - accuracy: 0.7520 - val_loss: 1.5757 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 106/500\n","\n","Epoch 106: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6346 - accuracy: 0.7607 - val_loss: 1.7785 - val_accuracy: 0.5100 - 3s/epoch - 18ms/step\n","Epoch 107/500\n","\n","Epoch 107: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6450 - accuracy: 0.7620 - val_loss: 1.6584 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 108/500\n","\n","Epoch 108: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6458 - accuracy: 0.7673 - val_loss: 1.5155 - val_accuracy: 0.5860 - 3s/epoch - 18ms/step\n","Epoch 109/500\n","\n","Epoch 109: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6455 - accuracy: 0.7573 - val_loss: 1.4733 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 110/500\n","\n","Epoch 110: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6275 - accuracy: 0.7660 - val_loss: 1.5271 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 111/500\n","\n","Epoch 111: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6447 - accuracy: 0.7620 - val_loss: 1.5217 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 112/500\n","\n","Epoch 112: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6534 - accuracy: 0.7582 - val_loss: 1.4972 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 113/500\n","\n","Epoch 113: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6422 - accuracy: 0.7633 - val_loss: 1.5092 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 114/500\n","\n","Epoch 114: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6617 - accuracy: 0.7553 - val_loss: 1.5735 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 115/500\n","\n","Epoch 115: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6243 - accuracy: 0.7653 - val_loss: 1.5553 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 116/500\n","\n","Epoch 116: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6515 - accuracy: 0.7629 - val_loss: 1.5913 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 117/500\n","\n","Epoch 117: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6548 - accuracy: 0.7611 - val_loss: 1.5299 - val_accuracy: 0.5520 - 3s/epoch - 18ms/step\n","Epoch 118/500\n","\n","Epoch 118: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6625 - accuracy: 0.7527 - val_loss: 1.6327 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 119/500\n","\n","Epoch 119: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6792 - accuracy: 0.7456 - val_loss: 1.4753 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 120/500\n","\n","Epoch 120: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6475 - accuracy: 0.7580 - val_loss: 1.5075 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 121/500\n","\n","Epoch 121: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6265 - accuracy: 0.7696 - val_loss: 1.4907 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 122/500\n","\n","Epoch 122: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6680 - accuracy: 0.7589 - val_loss: 1.6019 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 123/500\n","\n","Epoch 123: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6460 - accuracy: 0.7624 - val_loss: 1.5610 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 124/500\n","\n","Epoch 124: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6439 - accuracy: 0.7642 - val_loss: 1.5764 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 125/500\n","\n","Epoch 125: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6404 - accuracy: 0.7624 - val_loss: 1.5207 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 126/500\n","\n","Epoch 126: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6555 - accuracy: 0.7582 - val_loss: 1.5822 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 127/500\n","\n","Epoch 127: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6504 - accuracy: 0.7602 - val_loss: 1.5332 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 128/500\n","\n","Epoch 128: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6231 - accuracy: 0.7693 - val_loss: 1.7062 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 129/500\n","\n","Epoch 129: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6557 - accuracy: 0.7611 - val_loss: 1.5018 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 130/500\n","\n","Epoch 130: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6401 - accuracy: 0.7640 - val_loss: 1.5097 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 131/500\n","\n","Epoch 131: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6435 - accuracy: 0.7591 - val_loss: 1.5072 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 132/500\n","\n","Epoch 132: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6569 - accuracy: 0.7564 - val_loss: 1.5075 - val_accuracy: 0.5700 - 3s/epoch - 18ms/step\n","Epoch 133/500\n","\n","Epoch 133: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6320 - accuracy: 0.7756 - val_loss: 1.5329 - val_accuracy: 0.5700 - 3s/epoch - 18ms/step\n","Epoch 134/500\n","\n","Epoch 134: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6324 - accuracy: 0.7671 - val_loss: 1.5125 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 135/500\n","\n","Epoch 135: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6208 - accuracy: 0.7733 - val_loss: 1.5767 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 136/500\n","\n","Epoch 136: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6477 - accuracy: 0.7578 - val_loss: 1.4824 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 137/500\n","\n","Epoch 137: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6371 - accuracy: 0.7618 - val_loss: 1.5345 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 138/500\n","\n","Epoch 138: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6487 - accuracy: 0.7540 - val_loss: 1.4977 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 139/500\n","\n","Epoch 139: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6275 - accuracy: 0.7631 - val_loss: 1.4757 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 140/500\n","\n","Epoch 140: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6489 - accuracy: 0.7633 - val_loss: 1.5757 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 141/500\n","\n","Epoch 141: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6274 - accuracy: 0.7736 - val_loss: 1.5200 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 142/500\n","\n","Epoch 142: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6374 - accuracy: 0.7640 - val_loss: 1.5204 - val_accuracy: 0.5800 - 3s/epoch - 18ms/step\n","Epoch 143/500\n","\n","Epoch 143: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6589 - accuracy: 0.7531 - val_loss: 1.5077 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 144/500\n","\n","Epoch 144: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6412 - accuracy: 0.7649 - val_loss: 1.5364 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 145/500\n","\n","Epoch 145: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6374 - accuracy: 0.7640 - val_loss: 1.5183 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 146/500\n","\n","Epoch 146: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6262 - accuracy: 0.7678 - val_loss: 1.5853 - val_accuracy: 0.5520 - 2s/epoch - 18ms/step\n","Epoch 147/500\n","\n","Epoch 147: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6281 - accuracy: 0.7653 - val_loss: 1.4880 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 148/500\n","\n","Epoch 148: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6589 - accuracy: 0.7564 - val_loss: 1.4778 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 149/500\n","\n","Epoch 149: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6338 - accuracy: 0.7624 - val_loss: 1.5114 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 150/500\n","\n","Epoch 150: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6550 - accuracy: 0.7671 - val_loss: 1.4674 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 151/500\n","\n","Epoch 151: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6136 - accuracy: 0.7767 - val_loss: 1.6623 - val_accuracy: 0.5420 - 3s/epoch - 18ms/step\n","Epoch 152/500\n","\n","Epoch 152: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6238 - accuracy: 0.7658 - val_loss: 1.4945 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 153/500\n","\n","Epoch 153: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6216 - accuracy: 0.7691 - val_loss: 1.5548 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 154/500\n","\n","Epoch 154: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6339 - accuracy: 0.7687 - val_loss: 1.5301 - val_accuracy: 0.5820 - 2s/epoch - 18ms/step\n","Epoch 155/500\n","\n","Epoch 155: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6183 - accuracy: 0.7738 - val_loss: 1.5675 - val_accuracy: 0.5560 - 3s/epoch - 18ms/step\n","Epoch 156/500\n","\n","Epoch 156: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6500 - accuracy: 0.7638 - val_loss: 1.5332 - val_accuracy: 0.5700 - 3s/epoch - 18ms/step\n","Epoch 157/500\n","\n","Epoch 157: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6374 - accuracy: 0.7618 - val_loss: 1.5880 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 158/500\n","\n","Epoch 158: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6429 - accuracy: 0.7676 - val_loss: 1.5698 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 159/500\n","\n","Epoch 159: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6472 - accuracy: 0.7618 - val_loss: 1.5001 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 160/500\n","\n","Epoch 160: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6255 - accuracy: 0.7671 - val_loss: 1.4540 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 161/500\n","\n","Epoch 161: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6338 - accuracy: 0.7647 - val_loss: 1.5699 - val_accuracy: 0.5540 - 3s/epoch - 18ms/step\n","Epoch 162/500\n","\n","Epoch 162: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6331 - accuracy: 0.7607 - val_loss: 1.5543 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 163/500\n","\n","Epoch 163: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6393 - accuracy: 0.7647 - val_loss: 1.5174 - val_accuracy: 0.5780 - 3s/epoch - 18ms/step\n","Epoch 164/500\n","\n","Epoch 164: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6484 - accuracy: 0.7644 - val_loss: 1.6336 - val_accuracy: 0.5400 - 3s/epoch - 18ms/step\n","Epoch 165/500\n","\n","Epoch 165: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6560 - accuracy: 0.7569 - val_loss: 1.6000 - val_accuracy: 0.5320 - 3s/epoch - 18ms/step\n","Epoch 166/500\n","\n","Epoch 166: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6303 - accuracy: 0.7700 - val_loss: 1.5699 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 167/500\n","\n","Epoch 167: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6266 - accuracy: 0.7640 - val_loss: 1.5559 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 168/500\n","\n","Epoch 168: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6090 - accuracy: 0.7753 - val_loss: 1.5630 - val_accuracy: 0.5440 - 3s/epoch - 18ms/step\n","Epoch 169/500\n","\n","Epoch 169: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6369 - accuracy: 0.7644 - val_loss: 1.5302 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 170/500\n","\n","Epoch 170: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6483 - accuracy: 0.7651 - val_loss: 1.5332 - val_accuracy: 0.5480 - 3s/epoch - 18ms/step\n","Epoch 171/500\n","\n","Epoch 171: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6336 - accuracy: 0.7644 - val_loss: 1.5063 - val_accuracy: 0.5540 - 3s/epoch - 18ms/step\n","Epoch 172/500\n","\n","Epoch 172: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6176 - accuracy: 0.7720 - val_loss: 1.5679 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 173/500\n","\n","Epoch 173: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6338 - accuracy: 0.7673 - val_loss: 1.4935 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 174/500\n","\n","Epoch 174: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6371 - accuracy: 0.7647 - val_loss: 1.5817 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 175/500\n","\n","Epoch 175: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6322 - accuracy: 0.7656 - val_loss: 1.4673 - val_accuracy: 0.5780 - 3s/epoch - 19ms/step\n","Epoch 176/500\n","\n","Epoch 176: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6358 - accuracy: 0.7613 - val_loss: 1.5268 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 177/500\n","\n","Epoch 177: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6180 - accuracy: 0.7718 - val_loss: 1.5730 - val_accuracy: 0.5380 - 3s/epoch - 19ms/step\n","Epoch 178/500\n","\n","Epoch 178: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6311 - accuracy: 0.7680 - val_loss: 1.5331 - val_accuracy: 0.5500 - 3s/epoch - 18ms/step\n","Epoch 179/500\n","\n","Epoch 179: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6143 - accuracy: 0.7753 - val_loss: 1.6531 - val_accuracy: 0.5540 - 3s/epoch - 18ms/step\n","Epoch 180/500\n","\n","Epoch 180: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6221 - accuracy: 0.7709 - val_loss: 1.6221 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 181/500\n","\n","Epoch 181: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6081 - accuracy: 0.7831 - val_loss: 1.5666 - val_accuracy: 0.5660 - 3s/epoch - 19ms/step\n","Epoch 182/500\n","\n","Epoch 182: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6317 - accuracy: 0.7642 - val_loss: 1.5181 - val_accuracy: 0.5720 - 3s/epoch - 18ms/step\n","Epoch 183/500\n","\n","Epoch 183: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6409 - accuracy: 0.7660 - val_loss: 1.4868 - val_accuracy: 0.5660 - 3s/epoch - 18ms/step\n","Epoch 184/500\n","\n","Epoch 184: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6309 - accuracy: 0.7642 - val_loss: 1.4537 - val_accuracy: 0.5600 - 3s/epoch - 18ms/step\n","Epoch 185/500\n","\n","Epoch 185: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6274 - accuracy: 0.7744 - val_loss: 1.4603 - val_accuracy: 0.5760 - 3s/epoch - 18ms/step\n","Epoch 186/500\n","\n","Epoch 186: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6255 - accuracy: 0.7698 - val_loss: 1.5323 - val_accuracy: 0.5740 - 3s/epoch - 18ms/step\n","Epoch 187/500\n","\n","Epoch 187: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6197 - accuracy: 0.7664 - val_loss: 1.4903 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 188/500\n","\n","Epoch 188: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6231 - accuracy: 0.7642 - val_loss: 1.5575 - val_accuracy: 0.5580 - 3s/epoch - 18ms/step\n","Epoch 189/500\n","\n","Epoch 189: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6092 - accuracy: 0.7751 - val_loss: 1.6641 - val_accuracy: 0.5620 - 3s/epoch - 18ms/step\n","Epoch 190/500\n","\n","Epoch 190: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6219 - accuracy: 0.7727 - val_loss: 1.5347 - val_accuracy: 0.5640 - 3s/epoch - 18ms/step\n","Epoch 191/500\n","\n","Epoch 191: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6264 - accuracy: 0.7749 - val_loss: 1.6013 - val_accuracy: 0.5420 - 3s/epoch - 18ms/step\n","Epoch 192/500\n","\n","Epoch 192: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6220 - accuracy: 0.7756 - val_loss: 1.6101 - val_accuracy: 0.5520 - 3s/epoch - 18ms/step\n","Epoch 193/500\n","\n","Epoch 193: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6049 - accuracy: 0.7678 - val_loss: 1.5804 - val_accuracy: 0.5460 - 3s/epoch - 18ms/step\n","Epoch 194/500\n","\n","Epoch 194: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6284 - accuracy: 0.7660 - val_loss: 1.5927 - val_accuracy: 0.5560 - 3s/epoch - 18ms/step\n","Epoch 195/500\n","\n","Epoch 195: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6251 - accuracy: 0.7667 - val_loss: 1.5444 - val_accuracy: 0.5520 - 3s/epoch - 20ms/step\n","Epoch 196/500\n","\n","Epoch 196: val_accuracy did not improve from 0.60200\n","141/141 - 3s - loss: 0.6164 - accuracy: 0.7671 - val_loss: 1.5768 - val_accuracy: 0.5480 - 3s/epoch - 19ms/step\n","Epoch 197/500\n","\n","Epoch 197: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6146 - accuracy: 0.7756 - val_loss: 1.5245 - val_accuracy: 0.5620 - 2s/epoch - 16ms/step\n","Epoch 198/500\n","\n","Epoch 198: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6145 - accuracy: 0.7744 - val_loss: 1.5633 - val_accuracy: 0.5460 - 2s/epoch - 16ms/step\n","Epoch 199/500\n","\n","Epoch 199: val_accuracy did not improve from 0.60200\n","141/141 - 2s - loss: 0.6249 - accuracy: 0.7740 - val_loss: 1.6636 - val_accuracy: 0.5540 - 2s/epoch - 17ms/step\n","Epoch 200/500\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[20], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model with augmented data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py:1555\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1552\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()\n\u001b[0;32m   1554\u001b[0m     )\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1556\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m         ):\n\u001b[0;32m   1563\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1375\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1376\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n","File \u001b[1;32mc:\\Users\\monar\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from gc import callbacks\n","from random import shuffle\n","from tabnanny import verbose\n","from keras.preprocessing.image import ImageDataGenerator\n","# Data augmentation to generate more diverse training examples\n","datagen = ImageDataGenerator(\n","    rotation_range=20,        # Randomly rotate images by 20 degrees\n","    width_shift_range=0.2,    # Randomly shift the image horizontally\n","    height_shift_range=0.2,   # Randomly shift the image vertically\n","    zoom_range=0.2,           # Randomly zoom in or out\n","    horizontal_flip=True,     # Randomly flip images horizontally\n",")\n","\n","# Train the model with augmented data\n","epochs = 500\n","\n","history = model.fit(\n","    datagen.flow(X_train, y_train),\n","    epochs=epochs,\n","    validation_data=(X_test, y_test),batch_size = 512,callbacks= [stop,checkpoint],shuffle=True,verbose = 2\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:48:10.632942Z","iopub.status.busy":"2024-09-10T20:48:10.632375Z","iopub.status.idle":"2024-09-10T20:48:11.882213Z","shell.execute_reply":"2024-09-10T20:48:11.880867Z","shell.execute_reply.started":"2024-09-10T20:48:10.632893Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 7s 19ms/step - loss: 1.4343 - accuracy: 0.6020\n","Test Accuracy: 0.6020\n","16/16 [==============================] - 0s 4ms/step\n"]}],"source":["# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f'Test Accuracy: {accuracy:.4f}')\n","\n","# Predict emotions on the test set\n","predictions = model.predict(X_test)\n","predicted_labels = np.argmax(predictions, axis=1)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:13:59.020926Z","iopub.status.busy":"2024-09-10T20:13:59.020414Z","iopub.status.idle":"2024-09-10T20:14:03.637773Z","shell.execute_reply":"2024-09-10T20:14:03.636436Z","shell.execute_reply.started":"2024-09-10T20:13:59.020883Z"},"trusted":true},"outputs":[],"source":["test=test.apply(pd.to_numeric)\n","test=test.values\n","image_size = 48\n","test = test.reshape(-1, image_size, image_size, 1)  # Reshape to 48x48 images with 1 channel (grayscale)\n","# Normalize pixel values to [0, 1]\n","test = test / 255.0"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T20:48:23.947069Z","iopub.status.busy":"2024-09-10T20:48:23.946548Z","iopub.status.idle":"2024-09-10T20:48:25.561704Z","shell.execute_reply":"2024-09-10T20:48:25.560507Z","shell.execute_reply.started":"2024-09-10T20:48:23.947024Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["79/79 [==============================] - 0s 6ms/step\n","Submission saved to 'submission1.csv'.\n"]}],"source":["# Predict emotions on the test set\n","predictions = model.predict(test)\n","predicted_labels = np.argmax(predictions, axis=1)  # Get the class with the highest probability\n","\n","# Assuming you have a corresponding 'id' column in your test data\n","# We need to extract the 'id' from the original dataset corresponding to X_test\n","# First, make sure to retrieve the correct indices from the original data split\n","# Convert predictions and ids to DataFrame\n","submission_df = pd.DataFrame({\n","    'id': id,\n","    'emotion': predicted_labels\n","})\n","\n","# Save predictions to CSV file\n","submission_df.to_csv('submission1.csv', index=False)\n","\n","print(\"Submission saved to 'submission1.csv'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9530234,"sourceId":84797,"sourceType":"competition"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
