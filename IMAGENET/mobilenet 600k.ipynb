{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84797,"databundleVersionId":9530234,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-09-16T17:04:09.431644Z","iopub.execute_input":"2024-09-16T17:04:09.431998Z","iopub.status.idle":"2024-09-16T17:04:09.854931Z","shell.execute_reply.started":"2024-09-16T17:04:09.431962Z","shell.execute_reply":"2024-09-16T17:04:09.853696Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/emo-map-challenge/sample_submission.csv\n/kaggle/input/emo-map-challenge/train_dataset.csv\n/kaggle/input/emo-map-challenge/test_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:09.857247Z","iopub.execute_input":"2024-09-16T17:04:09.858167Z","iopub.status.idle":"2024-09-16T17:04:09.862797Z","shell.execute_reply.started":"2024-09-16T17:04:09.858107Z","shell.execute_reply":"2024-09-16T17:04:09.861817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/emo-map-challenge/train_dataset.csv')\ntest=pd.read_csv('/kaggle/input/emo-map-challenge/test_dataset.csv')\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:09.864247Z","iopub.execute_input":"2024-09-16T17:04:09.864691Z","iopub.status.idle":"2024-09-16T17:04:11.235534Z","shell.execute_reply.started":"2024-09-16T17:04:09.864643Z","shell.execute_reply":"2024-09-16T17:04:11.234216Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        id                                             pixels  emotion\n0        1  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...        0\n1        2  151 150 147 155 148 133 111 140 170 174 182 15...        0\n2        3  231 212 156 164 174 138 161 173 182 200 106 38...        2\n3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...        4\n4        5  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...        6\n...    ...                                                ...      ...\n4995  4996  22 24 23 23 25 24 23 20 18 13 6 2 0 1 7 22 32 ...        3\n4996  4997  73 85 87 87 74 118 120 132 134 127 133 118 105...        3\n4997  4998  253 253 254 254 254 254 250 219 166 141 109 70...        6\n4998  4999  78 84 77 95 90 85 72 75 79 84 86 82 88 102 110...        6\n4999  5000  98 100 102 104 107 109 111 119 126 130 53 5 12...        3\n\n[5000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>pixels</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>4996</td>\n      <td>22 24 23 23 25 24 23 20 18 13 6 2 0 1 7 22 32 ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>4997</td>\n      <td>73 85 87 87 74 118 120 132 134 127 133 118 105...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>4998</td>\n      <td>253 253 254 254 254 254 250 219 166 141 109 70...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>4999</td>\n      <td>78 84 77 95 90 85 72 75 79 84 86 82 88 102 110...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>5000</td>\n      <td>98 100 102 104 107 109 111 119 126 130 53 5 12...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:11.238146Z","iopub.execute_input":"2024-09-16T17:04:11.238530Z","iopub.status.idle":"2024-09-16T17:04:11.260978Z","shell.execute_reply.started":"2024-09-16T17:04:11.238490Z","shell.execute_reply":"2024-09-16T17:04:11.259502Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   id       5000 non-null   int64 \n 1   pixels   5000 non-null   object\n 2   emotion  5000 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 117.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"y=train['emotion'].values\nX=train.drop(['emotion','id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:11.262730Z","iopub.execute_input":"2024-09-16T17:04:11.263146Z","iopub.status.idle":"2024-09-16T17:04:11.271325Z","shell.execute_reply.started":"2024-09-16T17:04:11.263104Z","shell.execute_reply":"2024-09-16T17:04:11.270085Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Split the 'pixels' column into multiple columns\npixels_split = X['pixels'].str.split(' ', expand=True)\n\n# Rename columns to pixel_1, pixel_2, ..., pixel_n\npixels_split.columns = [f'pixel_{i+1}' for i in range(pixels_split.shape[1])]\n\n# Concatenate the new pixel columns with the original dataframe\nX= pd.concat([X, pixels_split], axis=1)\n\n# Optional: Drop the original 'pixels' column\nX.drop(columns=['pixels'], inplace=True)\nX\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:11.272908Z","iopub.execute_input":"2024-09-16T17:04:11.273291Z","iopub.status.idle":"2024-09-16T17:04:15.043213Z","shell.execute_reply.started":"2024-09-16T17:04:11.273254Z","shell.execute_reply":"2024-09-16T17:04:15.041887Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     pixel_1 pixel_2 pixel_3 pixel_4 pixel_5 pixel_6 pixel_7 pixel_8 pixel_9  \\\n0         70      80      82      72      58      58      60      63      54   \n1        151     150     147     155     148     133     111     140     170   \n2        231     212     156     164     174     138     161     173     182   \n3         24      32      36      30      32      23      19      20      30   \n4          4       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n4995      22      24      23      23      25      24      23      20      18   \n4996      73      85      87      87      74     118     120     132     134   \n4997     253     253     254     254     254     254     250     219     166   \n4998      78      84      77      95      90      85      72      75      79   \n4999      98     100     102     104     107     109     111     119     126   \n\n     pixel_10  ... pixel_2295 pixel_2296 pixel_2297 pixel_2298 pixel_2299  \\\n0          58  ...        159        182        183        136        106   \n1         174  ...        105        108         95        108        102   \n2         200  ...        104        138        152        122        114   \n3          41  ...        174        126        132        132        133   \n4           0  ...         12         34         31         31         31   \n...       ...  ...        ...        ...        ...        ...        ...   \n4995       13  ...         50         36         36         36         36   \n4996      127  ...        111        107         70         32         43   \n4997      141  ...        210        215        220        220        219   \n4998       84  ...         92         89         99         59         17   \n4999      130  ...         69         87         94        100         95   \n\n     pixel_2300 pixel_2301 pixel_2302 pixel_2303 pixel_2304  \n0           116         95        106        109         82  \n1            67        171        193        183        184  \n2           101         97         88        110        152  \n3           136        139        142        143        142  \n4            27         31         30         29         30  \n...         ...        ...        ...        ...        ...  \n4995         36         37         37         36         36  \n4996         89        111        122         78         52  \n4997        220        220        223        222        219  \n4998         26         27         34         41         47  \n4999         74        123        162        168        195  \n\n[5000 rows x 2304 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel_1</th>\n      <th>pixel_2</th>\n      <th>pixel_3</th>\n      <th>pixel_4</th>\n      <th>pixel_5</th>\n      <th>pixel_6</th>\n      <th>pixel_7</th>\n      <th>pixel_8</th>\n      <th>pixel_9</th>\n      <th>pixel_10</th>\n      <th>...</th>\n      <th>pixel_2295</th>\n      <th>pixel_2296</th>\n      <th>pixel_2297</th>\n      <th>pixel_2298</th>\n      <th>pixel_2299</th>\n      <th>pixel_2300</th>\n      <th>pixel_2301</th>\n      <th>pixel_2302</th>\n      <th>pixel_2303</th>\n      <th>pixel_2304</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70</td>\n      <td>80</td>\n      <td>82</td>\n      <td>72</td>\n      <td>58</td>\n      <td>58</td>\n      <td>60</td>\n      <td>63</td>\n      <td>54</td>\n      <td>58</td>\n      <td>...</td>\n      <td>159</td>\n      <td>182</td>\n      <td>183</td>\n      <td>136</td>\n      <td>106</td>\n      <td>116</td>\n      <td>95</td>\n      <td>106</td>\n      <td>109</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>151</td>\n      <td>150</td>\n      <td>147</td>\n      <td>155</td>\n      <td>148</td>\n      <td>133</td>\n      <td>111</td>\n      <td>140</td>\n      <td>170</td>\n      <td>174</td>\n      <td>...</td>\n      <td>105</td>\n      <td>108</td>\n      <td>95</td>\n      <td>108</td>\n      <td>102</td>\n      <td>67</td>\n      <td>171</td>\n      <td>193</td>\n      <td>183</td>\n      <td>184</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>231</td>\n      <td>212</td>\n      <td>156</td>\n      <td>164</td>\n      <td>174</td>\n      <td>138</td>\n      <td>161</td>\n      <td>173</td>\n      <td>182</td>\n      <td>200</td>\n      <td>...</td>\n      <td>104</td>\n      <td>138</td>\n      <td>152</td>\n      <td>122</td>\n      <td>114</td>\n      <td>101</td>\n      <td>97</td>\n      <td>88</td>\n      <td>110</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>32</td>\n      <td>36</td>\n      <td>30</td>\n      <td>32</td>\n      <td>23</td>\n      <td>19</td>\n      <td>20</td>\n      <td>30</td>\n      <td>41</td>\n      <td>...</td>\n      <td>174</td>\n      <td>126</td>\n      <td>132</td>\n      <td>132</td>\n      <td>133</td>\n      <td>136</td>\n      <td>139</td>\n      <td>142</td>\n      <td>143</td>\n      <td>142</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>12</td>\n      <td>34</td>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n      <td>27</td>\n      <td>31</td>\n      <td>30</td>\n      <td>29</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>22</td>\n      <td>24</td>\n      <td>23</td>\n      <td>23</td>\n      <td>25</td>\n      <td>24</td>\n      <td>23</td>\n      <td>20</td>\n      <td>18</td>\n      <td>13</td>\n      <td>...</td>\n      <td>50</td>\n      <td>36</td>\n      <td>36</td>\n      <td>36</td>\n      <td>36</td>\n      <td>36</td>\n      <td>37</td>\n      <td>37</td>\n      <td>36</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>73</td>\n      <td>85</td>\n      <td>87</td>\n      <td>87</td>\n      <td>74</td>\n      <td>118</td>\n      <td>120</td>\n      <td>132</td>\n      <td>134</td>\n      <td>127</td>\n      <td>...</td>\n      <td>111</td>\n      <td>107</td>\n      <td>70</td>\n      <td>32</td>\n      <td>43</td>\n      <td>89</td>\n      <td>111</td>\n      <td>122</td>\n      <td>78</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>253</td>\n      <td>253</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>250</td>\n      <td>219</td>\n      <td>166</td>\n      <td>141</td>\n      <td>...</td>\n      <td>210</td>\n      <td>215</td>\n      <td>220</td>\n      <td>220</td>\n      <td>219</td>\n      <td>220</td>\n      <td>220</td>\n      <td>223</td>\n      <td>222</td>\n      <td>219</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>78</td>\n      <td>84</td>\n      <td>77</td>\n      <td>95</td>\n      <td>90</td>\n      <td>85</td>\n      <td>72</td>\n      <td>75</td>\n      <td>79</td>\n      <td>84</td>\n      <td>...</td>\n      <td>92</td>\n      <td>89</td>\n      <td>99</td>\n      <td>59</td>\n      <td>17</td>\n      <td>26</td>\n      <td>27</td>\n      <td>34</td>\n      <td>41</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>98</td>\n      <td>100</td>\n      <td>102</td>\n      <td>104</td>\n      <td>107</td>\n      <td>109</td>\n      <td>111</td>\n      <td>119</td>\n      <td>126</td>\n      <td>130</td>\n      <td>...</td>\n      <td>69</td>\n      <td>87</td>\n      <td>94</td>\n      <td>100</td>\n      <td>95</td>\n      <td>74</td>\n      <td>123</td>\n      <td>162</td>\n      <td>168</td>\n      <td>195</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 2304 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X=X.apply(pd.to_numeric)\nX=X.values","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:15.044733Z","iopub.execute_input":"2024-09-16T17:04:15.045217Z","iopub.status.idle":"2024-09-16T17:04:29.147947Z","shell.execute_reply.started":"2024-09-16T17:04:15.045165Z","shell.execute_reply":"2024-09-16T17:04:29.146995Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Split the 'pixels' column into multiple columns\npixels_split = test['pixels'].str.split(' ', expand=True)\n\n# Rename columns to pixel_1, pixel_2, ..., pixel_n\npixels_split.columns = [f'pixel_{i+1}' for i in range(pixels_split.shape[1])]\n\n# Concatenate the new pixel columns with the original dataframe\ntest= pd.concat([test, pixels_split], axis=1)\nid=test.id\n# Optional: Drop the original 'pixels' column\ntest.drop(columns=['pixels','id'], inplace=True)\ntest\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:29.149249Z","iopub.execute_input":"2024-09-16T17:04:29.149581Z","iopub.status.idle":"2024-09-16T17:04:31.281251Z","shell.execute_reply.started":"2024-09-16T17:04:29.149545Z","shell.execute_reply":"2024-09-16T17:04:31.280154Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"     pixel_1 pixel_2 pixel_3 pixel_4 pixel_5 pixel_6 pixel_7 pixel_8 pixel_9  \\\n0         80      81      77      69      66      59      70      89     112   \n1        226     226     226     217     203     189      97     149     193   \n2         98     112      43      41      46      47      67      37      27   \n3         35      38      29      25      21      29      35      32      41   \n4          4       1       5      19      14      15      21      50      73   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n2495      50      36      17      22      23      29      33      39      34   \n2496     178     174     172     173     181     188     191     194     196   \n2497      17      17      16      23      28      22      19      17      25   \n2498      30      28      28      29      31      30      42      68      79   \n2499      19      13      14      12      13      16      21      33      50   \n\n     pixel_10  ... pixel_2295 pixel_2296 pixel_2297 pixel_2298 pixel_2299  \\\n0         132  ...         70         68         67         56         64   \n1         193  ...         48         69        106        114         88   \n2          37  ...         62         48         29         28         33   \n3          49  ...        142        139        171        201        213   \n4          73  ...        145        151        157        160        157   \n...       ...  ...        ...        ...        ...        ...        ...   \n2495       37  ...        216        215        216        217        221   \n2496      199  ...        147        141        136        118         66   \n2497       26  ...        190        179        193        193        194   \n2498       81  ...         30         27         27         26         28   \n2499       57  ...        224        217        209        195        151   \n\n     pixel_2300 pixel_2301 pixel_2302 pixel_2303 pixel_2304  \n0            70         75         84         75         70  \n1            89         36         22         26        105  \n2            35         36         40         40         39  \n3           219        219        226        252        254  \n4           158        170        185        191        194  \n...         ...        ...        ...        ...        ...  \n2495        222        220        223        221        216  \n2496          7          0          0          0          0  \n2497        170        148        154        133        113  \n2498         35         35         35         30         28  \n2499         99        146        189        199        201  \n\n[2500 rows x 2304 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel_1</th>\n      <th>pixel_2</th>\n      <th>pixel_3</th>\n      <th>pixel_4</th>\n      <th>pixel_5</th>\n      <th>pixel_6</th>\n      <th>pixel_7</th>\n      <th>pixel_8</th>\n      <th>pixel_9</th>\n      <th>pixel_10</th>\n      <th>...</th>\n      <th>pixel_2295</th>\n      <th>pixel_2296</th>\n      <th>pixel_2297</th>\n      <th>pixel_2298</th>\n      <th>pixel_2299</th>\n      <th>pixel_2300</th>\n      <th>pixel_2301</th>\n      <th>pixel_2302</th>\n      <th>pixel_2303</th>\n      <th>pixel_2304</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80</td>\n      <td>81</td>\n      <td>77</td>\n      <td>69</td>\n      <td>66</td>\n      <td>59</td>\n      <td>70</td>\n      <td>89</td>\n      <td>112</td>\n      <td>132</td>\n      <td>...</td>\n      <td>70</td>\n      <td>68</td>\n      <td>67</td>\n      <td>56</td>\n      <td>64</td>\n      <td>70</td>\n      <td>75</td>\n      <td>84</td>\n      <td>75</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>226</td>\n      <td>226</td>\n      <td>226</td>\n      <td>217</td>\n      <td>203</td>\n      <td>189</td>\n      <td>97</td>\n      <td>149</td>\n      <td>193</td>\n      <td>193</td>\n      <td>...</td>\n      <td>48</td>\n      <td>69</td>\n      <td>106</td>\n      <td>114</td>\n      <td>88</td>\n      <td>89</td>\n      <td>36</td>\n      <td>22</td>\n      <td>26</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>98</td>\n      <td>112</td>\n      <td>43</td>\n      <td>41</td>\n      <td>46</td>\n      <td>47</td>\n      <td>67</td>\n      <td>37</td>\n      <td>27</td>\n      <td>37</td>\n      <td>...</td>\n      <td>62</td>\n      <td>48</td>\n      <td>29</td>\n      <td>28</td>\n      <td>33</td>\n      <td>35</td>\n      <td>36</td>\n      <td>40</td>\n      <td>40</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35</td>\n      <td>38</td>\n      <td>29</td>\n      <td>25</td>\n      <td>21</td>\n      <td>29</td>\n      <td>35</td>\n      <td>32</td>\n      <td>41</td>\n      <td>49</td>\n      <td>...</td>\n      <td>142</td>\n      <td>139</td>\n      <td>171</td>\n      <td>201</td>\n      <td>213</td>\n      <td>219</td>\n      <td>219</td>\n      <td>226</td>\n      <td>252</td>\n      <td>254</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>5</td>\n      <td>19</td>\n      <td>14</td>\n      <td>15</td>\n      <td>21</td>\n      <td>50</td>\n      <td>73</td>\n      <td>73</td>\n      <td>...</td>\n      <td>145</td>\n      <td>151</td>\n      <td>157</td>\n      <td>160</td>\n      <td>157</td>\n      <td>158</td>\n      <td>170</td>\n      <td>185</td>\n      <td>191</td>\n      <td>194</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2495</th>\n      <td>50</td>\n      <td>36</td>\n      <td>17</td>\n      <td>22</td>\n      <td>23</td>\n      <td>29</td>\n      <td>33</td>\n      <td>39</td>\n      <td>34</td>\n      <td>37</td>\n      <td>...</td>\n      <td>216</td>\n      <td>215</td>\n      <td>216</td>\n      <td>217</td>\n      <td>221</td>\n      <td>222</td>\n      <td>220</td>\n      <td>223</td>\n      <td>221</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>2496</th>\n      <td>178</td>\n      <td>174</td>\n      <td>172</td>\n      <td>173</td>\n      <td>181</td>\n      <td>188</td>\n      <td>191</td>\n      <td>194</td>\n      <td>196</td>\n      <td>199</td>\n      <td>...</td>\n      <td>147</td>\n      <td>141</td>\n      <td>136</td>\n      <td>118</td>\n      <td>66</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2497</th>\n      <td>17</td>\n      <td>17</td>\n      <td>16</td>\n      <td>23</td>\n      <td>28</td>\n      <td>22</td>\n      <td>19</td>\n      <td>17</td>\n      <td>25</td>\n      <td>26</td>\n      <td>...</td>\n      <td>190</td>\n      <td>179</td>\n      <td>193</td>\n      <td>193</td>\n      <td>194</td>\n      <td>170</td>\n      <td>148</td>\n      <td>154</td>\n      <td>133</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>2498</th>\n      <td>30</td>\n      <td>28</td>\n      <td>28</td>\n      <td>29</td>\n      <td>31</td>\n      <td>30</td>\n      <td>42</td>\n      <td>68</td>\n      <td>79</td>\n      <td>81</td>\n      <td>...</td>\n      <td>30</td>\n      <td>27</td>\n      <td>27</td>\n      <td>26</td>\n      <td>28</td>\n      <td>35</td>\n      <td>35</td>\n      <td>35</td>\n      <td>30</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2499</th>\n      <td>19</td>\n      <td>13</td>\n      <td>14</td>\n      <td>12</td>\n      <td>13</td>\n      <td>16</td>\n      <td>21</td>\n      <td>33</td>\n      <td>50</td>\n      <td>57</td>\n      <td>...</td>\n      <td>224</td>\n      <td>217</td>\n      <td>209</td>\n      <td>195</td>\n      <td>151</td>\n      <td>99</td>\n      <td>146</td>\n      <td>189</td>\n      <td>199</td>\n      <td>201</td>\n    </tr>\n  </tbody>\n</table>\n<p>2500 rows × 2304 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:31.282711Z","iopub.execute_input":"2024-09-16T17:04:31.283167Z","iopub.status.idle":"2024-09-16T17:04:45.543389Z","shell.execute_reply.started":"2024-09-16T17:04:31.283117Z","shell.execute_reply":"2024-09-16T17:04:45.542410Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"image_size = 48\nX = X.reshape(-1, image_size, image_size, 1)  # Reshape to 48x48 images with 1 channel (grayscale)\n# Normalize pixel values to [0, 1]\nX = X / 255.0\n\n# One-hot encode the labels (since they range from 0 to 6, indicating 7 classes)\ny = to_categorical(y, num_classes=7)\n\n# Check the shapes of X and y\nprint(f'Shape of X: {X.shape}')\nprint(f'Shape of y: {y.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:45.547794Z","iopub.execute_input":"2024-09-16T17:04:45.548419Z","iopub.status.idle":"2024-09-16T17:04:45.599336Z","shell.execute_reply.started":"2024-09-16T17:04:45.548381Z","shell.execute_reply":"2024-09-16T17:04:45.598201Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Shape of X: (5000, 48, 48, 1)\nShape of y: (5000, 7)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split the dataset into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:45.600800Z","iopub.execute_input":"2024-09-16T17:04:45.601202Z","iopub.status.idle":"2024-09-16T17:04:45.859540Z","shell.execute_reply.started":"2024-09-16T17:04:45.601165Z","shell.execute_reply":"2024-09-16T17:04:45.858551Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, DepthwiseConv2D, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D\nfrom tensorflow.keras.models import Model\n\n# Input layer with fixed 48x48 dimensions and 1 channel\ninputs = Input(shape=(48, 48, 1))\n\n# Initial Batch Normalization\nx = BatchNormalization()(inputs)\n\n# Initial Conv2D layer with 32 filters\nx = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\n\n# Depthwise Separable Convolution Block 1\nx = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(64, (1, 1), padding='same', activation='relu')(x)  # Pointwise convolution\nx = BatchNormalization()(x)\n\n# MaxPooling\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Depthwise Separable Convolution Block 2\nx = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(128, (1, 1), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\n\n# MaxPooling\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Depthwise Separable Convolution Block 3\nx = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(256, (1, 1), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\n\n# MaxPooling\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Depthwise Separable Convolution Block 4 (New Layer)\nx = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(512, (1, 1), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\n\n# MaxPooling\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Depthwise Separable Convolution Block 5 (New Layer)\nx = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = Conv2D(512, (1, 1), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\n\n# Spatial Dropout to prevent overfitting\nx = Dropout(0.4)(x)\n\n# Global Average Pooling and Dropout\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\n\n# Fully connected dense layers\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\n\n# Output layer for 7 classes (corresponding to 7 facial expressions)\noutput = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model(inputs, output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:45.860737Z","iopub.execute_input":"2024-09-16T17:04:45.861103Z","iopub.status.idle":"2024-09-16T17:04:47.065369Z","shell.execute_reply.started":"2024-09-16T17:04:45.861066Z","shell.execute_reply":"2024-09-16T17:04:47.064289Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m4\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m2,112\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m8,320\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m1,280\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m33,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m2,560\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │       \u001b[38;5;34m131,584\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m5,120\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m903\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m623,051\u001b[0m (2.38 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">623,051</span> (2.38 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m618,057\u001b[0m (2.36 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">618,057</span> (2.36 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,994\u001b[0m (19.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,994</span> (19.51 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"import keras\ncheckpoint = keras.callbacks.ModelCheckpoint('ckpt2.keras',monitor = 'val_accuracy', verbose = 1, save_best_only=True, mode ='max')\n\nstop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 300)\ntry:\n model.load_weights('ckpt2.keras')\nexcept:print('creating new')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:47.066551Z","iopub.execute_input":"2024-09-16T17:04:47.066919Z","iopub.status.idle":"2024-09-16T17:04:47.074309Z","shell.execute_reply.started":"2024-09-16T17:04:47.066875Z","shell.execute_reply":"2024-09-16T17:04:47.073060Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"creating new\n","output_type":"stream"}]},{"cell_type":"code","source":"#import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rotation_range=20,        # Randomly rotate images by 20 degrees\n    width_shift_range=0.2,    # Randomly shift the image horizontally\n    height_shift_range=0.2,   # Randomly shift the image vertically\n    zoom_range=0.2,           # Randomly zoom in or out\n    horizontal_flip=True,     # Randomly flip images horizontally\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:47.075583Z","iopub.execute_input":"2024-09-16T17:04:47.075952Z","iopub.status.idle":"2024-09-16T17:04:47.087008Z","shell.execute_reply.started":"2024-09-16T17:04:47.075917Z","shell.execute_reply":"2024-09-16T17:04:47.086137Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Train the model\nfrom random import shuffle\n\n\nmodel.fit( datagen.flow(X_train, y_train), epochs=500,verbose=2, validation_data=(X_test, y_test),callbacks=[stop,checkpoint],shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:04:47.088383Z","iopub.execute_input":"2024-09-16T17:04:47.088731Z","iopub.status.idle":"2024-09-16T17:24:58.855683Z","shell.execute_reply.started":"2024-09-16T17:04:47.088695Z","shell.execute_reply":"2024-09-16T17:24:58.854583Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/500\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1726506296.007031     106 service.cc:145] XLA service 0x7e49e0009a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1726506296.007124     106 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1726506296.007131     106 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1726506311.313374     106 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_accuracy improved from -inf to 0.27200, saving model to ckpt2.keras\n141/141 - 42s - 299ms/step - accuracy: 0.2000 - loss: 2.0205 - val_accuracy: 0.2720 - val_loss: 1.8391\nEpoch 2/500\n\nEpoch 2: val_accuracy did not improve from 0.27200\n141/141 - 10s - 73ms/step - accuracy: 0.2111 - loss: 1.8793 - val_accuracy: 0.2720 - val_loss: 1.7966\nEpoch 3/500\n\nEpoch 3: val_accuracy did not improve from 0.27200\n141/141 - 3s - 20ms/step - accuracy: 0.2260 - loss: 1.8451 - val_accuracy: 0.2720 - val_loss: 1.8065\nEpoch 4/500\n\nEpoch 4: val_accuracy did not improve from 0.27200\n141/141 - 3s - 20ms/step - accuracy: 0.2400 - loss: 1.8229 - val_accuracy: 0.2720 - val_loss: 1.7919\nEpoch 5/500\n\nEpoch 5: val_accuracy improved from 0.27200 to 0.27400, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2391 - loss: 1.8173 - val_accuracy: 0.2740 - val_loss: 1.7737\nEpoch 6/500\n\nEpoch 6: val_accuracy did not improve from 0.27400\n141/141 - 3s - 20ms/step - accuracy: 0.2420 - loss: 1.8186 - val_accuracy: 0.2740 - val_loss: 1.7746\nEpoch 7/500\n\nEpoch 7: val_accuracy improved from 0.27400 to 0.28000, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2371 - loss: 1.8005 - val_accuracy: 0.2800 - val_loss: 1.7638\nEpoch 8/500\n\nEpoch 8: val_accuracy improved from 0.28000 to 0.28600, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2547 - loss: 1.7916 - val_accuracy: 0.2860 - val_loss: 1.7510\nEpoch 9/500\n\nEpoch 9: val_accuracy did not improve from 0.28600\n141/141 - 3s - 20ms/step - accuracy: 0.2564 - loss: 1.7945 - val_accuracy: 0.2620 - val_loss: 1.7589\nEpoch 10/500\n\nEpoch 10: val_accuracy improved from 0.28600 to 0.28800, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2616 - loss: 1.7912 - val_accuracy: 0.2880 - val_loss: 1.7620\nEpoch 11/500\n\nEpoch 11: val_accuracy improved from 0.28800 to 0.29200, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2509 - loss: 1.7816 - val_accuracy: 0.2920 - val_loss: 1.7480\nEpoch 12/500\n\nEpoch 12: val_accuracy improved from 0.29200 to 0.30600, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2598 - loss: 1.7771 - val_accuracy: 0.3060 - val_loss: 1.7066\nEpoch 13/500\n\nEpoch 13: val_accuracy improved from 0.30600 to 0.31800, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2698 - loss: 1.7688 - val_accuracy: 0.3180 - val_loss: 1.7282\nEpoch 14/500\n\nEpoch 14: val_accuracy did not improve from 0.31800\n141/141 - 3s - 20ms/step - accuracy: 0.2778 - loss: 1.7581 - val_accuracy: 0.3060 - val_loss: 1.6898\nEpoch 15/500\n\nEpoch 15: val_accuracy did not improve from 0.31800\n141/141 - 3s - 21ms/step - accuracy: 0.2807 - loss: 1.7463 - val_accuracy: 0.3040 - val_loss: 1.7187\nEpoch 16/500\n\nEpoch 16: val_accuracy improved from 0.31800 to 0.34600, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.2889 - loss: 1.7255 - val_accuracy: 0.3460 - val_loss: 1.6180\nEpoch 17/500\n\nEpoch 17: val_accuracy did not improve from 0.34600\n141/141 - 3s - 21ms/step - accuracy: 0.2942 - loss: 1.7217 - val_accuracy: 0.3140 - val_loss: 1.7291\nEpoch 18/500\n\nEpoch 18: val_accuracy improved from 0.34600 to 0.36000, saving model to ckpt2.keras\n141/141 - 3s - 23ms/step - accuracy: 0.2902 - loss: 1.7173 - val_accuracy: 0.3600 - val_loss: 1.6069\nEpoch 19/500\n\nEpoch 19: val_accuracy improved from 0.36000 to 0.36800, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.3044 - loss: 1.7064 - val_accuracy: 0.3680 - val_loss: 1.5993\nEpoch 20/500\n\nEpoch 20: val_accuracy did not improve from 0.36800\n141/141 - 3s - 20ms/step - accuracy: 0.3156 - loss: 1.6872 - val_accuracy: 0.3640 - val_loss: 1.5647\nEpoch 21/500\n\nEpoch 21: val_accuracy did not improve from 0.36800\n141/141 - 3s - 20ms/step - accuracy: 0.3227 - loss: 1.6859 - val_accuracy: 0.3420 - val_loss: 1.5974\nEpoch 22/500\n\nEpoch 22: val_accuracy improved from 0.36800 to 0.37000, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.3258 - loss: 1.6729 - val_accuracy: 0.3700 - val_loss: 1.6191\nEpoch 23/500\n\nEpoch 23: val_accuracy improved from 0.37000 to 0.39200, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.3196 - loss: 1.6687 - val_accuracy: 0.3920 - val_loss: 1.5484\nEpoch 24/500\n\nEpoch 24: val_accuracy improved from 0.39200 to 0.40400, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.3409 - loss: 1.6444 - val_accuracy: 0.4040 - val_loss: 1.5576\nEpoch 25/500\n\nEpoch 25: val_accuracy did not improve from 0.40400\n141/141 - 3s - 21ms/step - accuracy: 0.3438 - loss: 1.6299 - val_accuracy: 0.4040 - val_loss: 1.5028\nEpoch 26/500\n\nEpoch 26: val_accuracy did not improve from 0.40400\n141/141 - 3s - 20ms/step - accuracy: 0.3456 - loss: 1.6267 - val_accuracy: 0.3920 - val_loss: 1.5779\nEpoch 27/500\n\nEpoch 27: val_accuracy improved from 0.40400 to 0.42600, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.3533 - loss: 1.6161 - val_accuracy: 0.4260 - val_loss: 1.4797\nEpoch 28/500\n\nEpoch 28: val_accuracy did not improve from 0.42600\n141/141 - 3s - 21ms/step - accuracy: 0.3624 - loss: 1.6027 - val_accuracy: 0.4020 - val_loss: 1.5381\nEpoch 29/500\n\nEpoch 29: val_accuracy did not improve from 0.42600\n141/141 - 3s - 21ms/step - accuracy: 0.3687 - loss: 1.5866 - val_accuracy: 0.4220 - val_loss: 1.5035\nEpoch 30/500\n\nEpoch 30: val_accuracy did not improve from 0.42600\n141/141 - 3s - 20ms/step - accuracy: 0.3693 - loss: 1.5801 - val_accuracy: 0.3820 - val_loss: 1.5442\nEpoch 31/500\n\nEpoch 31: val_accuracy improved from 0.42600 to 0.44400, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.3867 - loss: 1.5605 - val_accuracy: 0.4440 - val_loss: 1.4592\nEpoch 32/500\n\nEpoch 32: val_accuracy improved from 0.44400 to 0.46600, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.3824 - loss: 1.5573 - val_accuracy: 0.4660 - val_loss: 1.4176\nEpoch 33/500\n\nEpoch 33: val_accuracy did not improve from 0.46600\n141/141 - 3s - 20ms/step - accuracy: 0.3836 - loss: 1.5506 - val_accuracy: 0.4600 - val_loss: 1.4071\nEpoch 34/500\n\nEpoch 34: val_accuracy did not improve from 0.46600\n141/141 - 3s - 20ms/step - accuracy: 0.3820 - loss: 1.5378 - val_accuracy: 0.4440 - val_loss: 1.3880\nEpoch 35/500\n\nEpoch 35: val_accuracy did not improve from 0.46600\n141/141 - 3s - 20ms/step - accuracy: 0.3931 - loss: 1.5141 - val_accuracy: 0.4560 - val_loss: 1.4760\nEpoch 36/500\n\nEpoch 36: val_accuracy improved from 0.46600 to 0.47000, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.4038 - loss: 1.5228 - val_accuracy: 0.4700 - val_loss: 1.4070\nEpoch 37/500\n\nEpoch 37: val_accuracy did not improve from 0.47000\n141/141 - 3s - 20ms/step - accuracy: 0.4040 - loss: 1.5287 - val_accuracy: 0.4600 - val_loss: 1.4688\nEpoch 38/500\n\nEpoch 38: val_accuracy improved from 0.47000 to 0.47200, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.4109 - loss: 1.5172 - val_accuracy: 0.4720 - val_loss: 1.4341\nEpoch 39/500\n\nEpoch 39: val_accuracy improved from 0.47200 to 0.49600, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.4087 - loss: 1.5088 - val_accuracy: 0.4960 - val_loss: 1.3995\nEpoch 40/500\n\nEpoch 40: val_accuracy did not improve from 0.49600\n141/141 - 5s - 34ms/step - accuracy: 0.4160 - loss: 1.4960 - val_accuracy: 0.4900 - val_loss: 1.3881\nEpoch 41/500\n\nEpoch 41: val_accuracy did not improve from 0.49600\n141/141 - 3s - 19ms/step - accuracy: 0.4247 - loss: 1.4718 - val_accuracy: 0.4740 - val_loss: 1.4198\nEpoch 42/500\n\nEpoch 42: val_accuracy did not improve from 0.49600\n141/141 - 3s - 20ms/step - accuracy: 0.4147 - loss: 1.4782 - val_accuracy: 0.4580 - val_loss: 1.4851\nEpoch 43/500\n\nEpoch 43: val_accuracy improved from 0.49600 to 0.50800, saving model to ckpt2.keras\n141/141 - 3s - 20ms/step - accuracy: 0.4344 - loss: 1.4661 - val_accuracy: 0.5080 - val_loss: 1.3869\nEpoch 44/500\n\nEpoch 44: val_accuracy did not improve from 0.50800\n141/141 - 3s - 19ms/step - accuracy: 0.4416 - loss: 1.4679 - val_accuracy: 0.4900 - val_loss: 1.3860\nEpoch 45/500\n\nEpoch 45: val_accuracy did not improve from 0.50800\n141/141 - 3s - 20ms/step - accuracy: 0.4344 - loss: 1.4613 - val_accuracy: 0.4720 - val_loss: 1.3634\nEpoch 46/500\n\nEpoch 46: val_accuracy did not improve from 0.50800\n141/141 - 3s - 19ms/step - accuracy: 0.4382 - loss: 1.4487 - val_accuracy: 0.5040 - val_loss: 1.3580\nEpoch 47/500\n\nEpoch 47: val_accuracy did not improve from 0.50800\n141/141 - 3s - 19ms/step - accuracy: 0.4313 - loss: 1.4515 - val_accuracy: 0.4820 - val_loss: 1.3978\nEpoch 48/500\n\nEpoch 48: val_accuracy did not improve from 0.50800\n141/141 - 3s - 19ms/step - accuracy: 0.4404 - loss: 1.4494 - val_accuracy: 0.4920 - val_loss: 1.3722\nEpoch 49/500\n\nEpoch 49: val_accuracy did not improve from 0.50800\n141/141 - 3s - 20ms/step - accuracy: 0.4478 - loss: 1.4210 - val_accuracy: 0.4880 - val_loss: 1.4198\nEpoch 50/500\n\nEpoch 50: val_accuracy did not improve from 0.50800\n141/141 - 3s - 20ms/step - accuracy: 0.4456 - loss: 1.4313 - val_accuracy: 0.5020 - val_loss: 1.3876\nEpoch 51/500\n\nEpoch 51: val_accuracy did not improve from 0.50800\n141/141 - 3s - 19ms/step - accuracy: 0.4507 - loss: 1.4031 - val_accuracy: 0.4820 - val_loss: 1.4214\nEpoch 52/500\n\nEpoch 52: val_accuracy improved from 0.50800 to 0.51400, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.4569 - loss: 1.4239 - val_accuracy: 0.5140 - val_loss: 1.3720\nEpoch 53/500\n\nEpoch 53: val_accuracy did not improve from 0.51400\n141/141 - 3s - 19ms/step - accuracy: 0.4556 - loss: 1.4149 - val_accuracy: 0.5000 - val_loss: 1.4021\nEpoch 54/500\n\nEpoch 54: val_accuracy did not improve from 0.51400\n141/141 - 3s - 19ms/step - accuracy: 0.4509 - loss: 1.4098 - val_accuracy: 0.4960 - val_loss: 1.3467\nEpoch 55/500\n\nEpoch 55: val_accuracy improved from 0.51400 to 0.52800, saving model to ckpt2.keras\n141/141 - 3s - 20ms/step - accuracy: 0.4607 - loss: 1.4071 - val_accuracy: 0.5280 - val_loss: 1.3008\nEpoch 56/500\n\nEpoch 56: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4547 - loss: 1.4050 - val_accuracy: 0.5160 - val_loss: 1.3309\nEpoch 57/500\n\nEpoch 57: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.4667 - loss: 1.3875 - val_accuracy: 0.4880 - val_loss: 1.3480\nEpoch 58/500\n\nEpoch 58: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.4749 - loss: 1.3758 - val_accuracy: 0.5120 - val_loss: 1.3588\nEpoch 59/500\n\nEpoch 59: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.4742 - loss: 1.3630 - val_accuracy: 0.4960 - val_loss: 1.3736\nEpoch 60/500\n\nEpoch 60: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4658 - loss: 1.3856 - val_accuracy: 0.5160 - val_loss: 1.3048\nEpoch 61/500\n\nEpoch 61: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.4684 - loss: 1.3707 - val_accuracy: 0.4760 - val_loss: 1.3738\nEpoch 62/500\n\nEpoch 62: val_accuracy did not improve from 0.52800\n141/141 - 3s - 21ms/step - accuracy: 0.4709 - loss: 1.3712 - val_accuracy: 0.5060 - val_loss: 1.3177\nEpoch 63/500\n\nEpoch 63: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4624 - loss: 1.3786 - val_accuracy: 0.4800 - val_loss: 1.3250\nEpoch 64/500\n\nEpoch 64: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4764 - loss: 1.3468 - val_accuracy: 0.5000 - val_loss: 1.3058\nEpoch 65/500\n\nEpoch 65: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.4769 - loss: 1.3473 - val_accuracy: 0.4780 - val_loss: 1.3817\nEpoch 66/500\n\nEpoch 66: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4844 - loss: 1.3466 - val_accuracy: 0.4880 - val_loss: 1.3331\nEpoch 67/500\n\nEpoch 67: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4889 - loss: 1.3431 - val_accuracy: 0.5180 - val_loss: 1.3114\nEpoch 68/500\n\nEpoch 68: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4773 - loss: 1.3364 - val_accuracy: 0.5060 - val_loss: 1.3600\nEpoch 69/500\n\nEpoch 69: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4989 - loss: 1.3369 - val_accuracy: 0.5120 - val_loss: 1.3066\nEpoch 70/500\n\nEpoch 70: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4880 - loss: 1.3445 - val_accuracy: 0.4920 - val_loss: 1.3583\nEpoch 71/500\n\nEpoch 71: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4860 - loss: 1.3250 - val_accuracy: 0.5080 - val_loss: 1.3358\nEpoch 72/500\n\nEpoch 72: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4918 - loss: 1.3224 - val_accuracy: 0.5040 - val_loss: 1.3076\nEpoch 73/500\n\nEpoch 73: val_accuracy did not improve from 0.52800\n141/141 - 3s - 21ms/step - accuracy: 0.4940 - loss: 1.3296 - val_accuracy: 0.5120 - val_loss: 1.2806\nEpoch 74/500\n\nEpoch 74: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.5016 - loss: 1.3225 - val_accuracy: 0.4900 - val_loss: 1.3365\nEpoch 75/500\n\nEpoch 75: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4960 - loss: 1.3224 - val_accuracy: 0.4880 - val_loss: 1.4137\nEpoch 76/500\n\nEpoch 76: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4960 - loss: 1.3164 - val_accuracy: 0.5180 - val_loss: 1.2712\nEpoch 77/500\n\nEpoch 77: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.4913 - loss: 1.3220 - val_accuracy: 0.5160 - val_loss: 1.2925\nEpoch 78/500\n\nEpoch 78: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.5024 - loss: 1.3092 - val_accuracy: 0.5100 - val_loss: 1.2984\nEpoch 79/500\n\nEpoch 79: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.5067 - loss: 1.2934 - val_accuracy: 0.5220 - val_loss: 1.3444\nEpoch 80/500\n\nEpoch 80: val_accuracy did not improve from 0.52800\n141/141 - 3s - 19ms/step - accuracy: 0.5071 - loss: 1.3043 - val_accuracy: 0.5100 - val_loss: 1.3134\nEpoch 81/500\n\nEpoch 81: val_accuracy did not improve from 0.52800\n141/141 - 3s - 20ms/step - accuracy: 0.5098 - loss: 1.2731 - val_accuracy: 0.5020 - val_loss: 1.3248\nEpoch 82/500\n\nEpoch 82: val_accuracy improved from 0.52800 to 0.53600, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.5073 - loss: 1.2875 - val_accuracy: 0.5360 - val_loss: 1.2601\nEpoch 83/500\n\nEpoch 83: val_accuracy did not improve from 0.53600\n141/141 - 3s - 20ms/step - accuracy: 0.5158 - loss: 1.2786 - val_accuracy: 0.5320 - val_loss: 1.3141\nEpoch 84/500\n\nEpoch 84: val_accuracy did not improve from 0.53600\n141/141 - 3s - 21ms/step - accuracy: 0.5060 - loss: 1.2744 - val_accuracy: 0.5020 - val_loss: 1.3465\nEpoch 85/500\n\nEpoch 85: val_accuracy did not improve from 0.53600\n141/141 - 3s - 20ms/step - accuracy: 0.5191 - loss: 1.2875 - val_accuracy: 0.5100 - val_loss: 1.3313\nEpoch 86/500\n\nEpoch 86: val_accuracy did not improve from 0.53600\n141/141 - 3s - 20ms/step - accuracy: 0.5093 - loss: 1.3014 - val_accuracy: 0.5120 - val_loss: 1.3260\nEpoch 87/500\n\nEpoch 87: val_accuracy did not improve from 0.53600\n141/141 - 3s - 20ms/step - accuracy: 0.5111 - loss: 1.2769 - val_accuracy: 0.5320 - val_loss: 1.2711\nEpoch 88/500\n\nEpoch 88: val_accuracy did not improve from 0.53600\n141/141 - 3s - 21ms/step - accuracy: 0.5120 - loss: 1.2597 - val_accuracy: 0.5080 - val_loss: 1.2522\nEpoch 89/500\n\nEpoch 89: val_accuracy did not improve from 0.53600\n141/141 - 3s - 20ms/step - accuracy: 0.5173 - loss: 1.2839 - val_accuracy: 0.5320 - val_loss: 1.2877\nEpoch 90/500\n\nEpoch 90: val_accuracy improved from 0.53600 to 0.54400, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.5164 - loss: 1.2656 - val_accuracy: 0.5440 - val_loss: 1.2630\nEpoch 91/500\n\nEpoch 91: val_accuracy did not improve from 0.54400\n141/141 - 3s - 21ms/step - accuracy: 0.5207 - loss: 1.2585 - val_accuracy: 0.5260 - val_loss: 1.2849\nEpoch 92/500\n\nEpoch 92: val_accuracy did not improve from 0.54400\n141/141 - 3s - 20ms/step - accuracy: 0.5207 - loss: 1.2589 - val_accuracy: 0.5420 - val_loss: 1.2682\nEpoch 93/500\n\nEpoch 93: val_accuracy did not improve from 0.54400\n141/141 - 3s - 20ms/step - accuracy: 0.5271 - loss: 1.2535 - val_accuracy: 0.5220 - val_loss: 1.3240\nEpoch 94/500\n\nEpoch 94: val_accuracy improved from 0.54400 to 0.54600, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.5289 - loss: 1.2422 - val_accuracy: 0.5460 - val_loss: 1.2471\nEpoch 95/500\n\nEpoch 95: val_accuracy did not improve from 0.54600\n141/141 - 3s - 22ms/step - accuracy: 0.5331 - loss: 1.2606 - val_accuracy: 0.5220 - val_loss: 1.2890\nEpoch 96/500\n\nEpoch 96: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5300 - loss: 1.2414 - val_accuracy: 0.5240 - val_loss: 1.3392\nEpoch 97/500\n\nEpoch 97: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5173 - loss: 1.2608 - val_accuracy: 0.5160 - val_loss: 1.2666\nEpoch 98/500\n\nEpoch 98: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5258 - loss: 1.2421 - val_accuracy: 0.5100 - val_loss: 1.3175\nEpoch 99/500\n\nEpoch 99: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5384 - loss: 1.2315 - val_accuracy: 0.5240 - val_loss: 1.2721\nEpoch 100/500\n\nEpoch 100: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5307 - loss: 1.2382 - val_accuracy: 0.5360 - val_loss: 1.2645\nEpoch 101/500\n\nEpoch 101: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5389 - loss: 1.2361 - val_accuracy: 0.5340 - val_loss: 1.2825\nEpoch 102/500\n\nEpoch 102: val_accuracy did not improve from 0.54600\n141/141 - 3s - 21ms/step - accuracy: 0.5371 - loss: 1.2393 - val_accuracy: 0.5240 - val_loss: 1.2484\nEpoch 103/500\n\nEpoch 103: val_accuracy did not improve from 0.54600\n141/141 - 3s - 20ms/step - accuracy: 0.5236 - loss: 1.2386 - val_accuracy: 0.5320 - val_loss: 1.2854\nEpoch 104/500\n\nEpoch 104: val_accuracy improved from 0.54600 to 0.55600, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.5342 - loss: 1.2338 - val_accuracy: 0.5560 - val_loss: 1.2314\nEpoch 105/500\n\nEpoch 105: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5313 - loss: 1.2319 - val_accuracy: 0.5300 - val_loss: 1.2477\nEpoch 106/500\n\nEpoch 106: val_accuracy did not improve from 0.55600\n141/141 - 3s - 21ms/step - accuracy: 0.5418 - loss: 1.2089 - val_accuracy: 0.5400 - val_loss: 1.2466\nEpoch 107/500\n\nEpoch 107: val_accuracy did not improve from 0.55600\n141/141 - 3s - 21ms/step - accuracy: 0.5484 - loss: 1.2078 - val_accuracy: 0.5320 - val_loss: 1.2261\nEpoch 108/500\n\nEpoch 108: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5380 - loss: 1.2224 - val_accuracy: 0.5180 - val_loss: 1.2941\nEpoch 109/500\n\nEpoch 109: val_accuracy did not improve from 0.55600\n141/141 - 3s - 21ms/step - accuracy: 0.5447 - loss: 1.2162 - val_accuracy: 0.5340 - val_loss: 1.2222\nEpoch 110/500\n\nEpoch 110: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5387 - loss: 1.2080 - val_accuracy: 0.5260 - val_loss: 1.2766\nEpoch 111/500\n\nEpoch 111: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5440 - loss: 1.2254 - val_accuracy: 0.5380 - val_loss: 1.2654\nEpoch 112/500\n\nEpoch 112: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5542 - loss: 1.2039 - val_accuracy: 0.5400 - val_loss: 1.2478\nEpoch 113/500\n\nEpoch 113: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5460 - loss: 1.1979 - val_accuracy: 0.5340 - val_loss: 1.2795\nEpoch 114/500\n\nEpoch 114: val_accuracy did not improve from 0.55600\n141/141 - 3s - 19ms/step - accuracy: 0.5480 - loss: 1.2004 - val_accuracy: 0.5280 - val_loss: 1.2665\nEpoch 115/500\n\nEpoch 115: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5429 - loss: 1.1941 - val_accuracy: 0.5180 - val_loss: 1.2995\nEpoch 116/500\n\nEpoch 116: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5527 - loss: 1.1985 - val_accuracy: 0.5320 - val_loss: 1.3343\nEpoch 117/500\n\nEpoch 117: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5509 - loss: 1.1988 - val_accuracy: 0.5440 - val_loss: 1.2790\nEpoch 118/500\n\nEpoch 118: val_accuracy did not improve from 0.55600\n141/141 - 5s - 36ms/step - accuracy: 0.5629 - loss: 1.1723 - val_accuracy: 0.5340 - val_loss: 1.2806\nEpoch 119/500\n\nEpoch 119: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5380 - loss: 1.1889 - val_accuracy: 0.5340 - val_loss: 1.2686\nEpoch 120/500\n\nEpoch 120: val_accuracy did not improve from 0.55600\n141/141 - 3s - 20ms/step - accuracy: 0.5727 - loss: 1.1658 - val_accuracy: 0.5480 - val_loss: 1.2464\nEpoch 121/500\n\nEpoch 121: val_accuracy improved from 0.55600 to 0.56400, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.5607 - loss: 1.1882 - val_accuracy: 0.5640 - val_loss: 1.2435\nEpoch 122/500\n\nEpoch 122: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5640 - loss: 1.1734 - val_accuracy: 0.5380 - val_loss: 1.2878\nEpoch 123/500\n\nEpoch 123: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5524 - loss: 1.1702 - val_accuracy: 0.5260 - val_loss: 1.2893\nEpoch 124/500\n\nEpoch 124: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5629 - loss: 1.1756 - val_accuracy: 0.5380 - val_loss: 1.2560\nEpoch 125/500\n\nEpoch 125: val_accuracy did not improve from 0.56400\n141/141 - 3s - 19ms/step - accuracy: 0.5613 - loss: 1.1780 - val_accuracy: 0.5320 - val_loss: 1.2738\nEpoch 126/500\n\nEpoch 126: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5651 - loss: 1.1511 - val_accuracy: 0.5280 - val_loss: 1.2930\nEpoch 127/500\n\nEpoch 127: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5536 - loss: 1.1740 - val_accuracy: 0.5440 - val_loss: 1.2648\nEpoch 128/500\n\nEpoch 128: val_accuracy did not improve from 0.56400\n141/141 - 3s - 21ms/step - accuracy: 0.5653 - loss: 1.1706 - val_accuracy: 0.5580 - val_loss: 1.2471\nEpoch 129/500\n\nEpoch 129: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5611 - loss: 1.1614 - val_accuracy: 0.5640 - val_loss: 1.2335\nEpoch 130/500\n\nEpoch 130: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5629 - loss: 1.1511 - val_accuracy: 0.5420 - val_loss: 1.2760\nEpoch 131/500\n\nEpoch 131: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5620 - loss: 1.1642 - val_accuracy: 0.5580 - val_loss: 1.2538\nEpoch 132/500\n\nEpoch 132: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5618 - loss: 1.1461 - val_accuracy: 0.5440 - val_loss: 1.2879\nEpoch 133/500\n\nEpoch 133: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5691 - loss: 1.1427 - val_accuracy: 0.5560 - val_loss: 1.2833\nEpoch 134/500\n\nEpoch 134: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5689 - loss: 1.1569 - val_accuracy: 0.5320 - val_loss: 1.3050\nEpoch 135/500\n\nEpoch 135: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5651 - loss: 1.1499 - val_accuracy: 0.5620 - val_loss: 1.2668\nEpoch 136/500\n\nEpoch 136: val_accuracy did not improve from 0.56400\n141/141 - 3s - 20ms/step - accuracy: 0.5769 - loss: 1.1381 - val_accuracy: 0.5460 - val_loss: 1.2440\nEpoch 137/500\n\nEpoch 137: val_accuracy did not improve from 0.56400\n141/141 - 3s - 19ms/step - accuracy: 0.5733 - loss: 1.1466 - val_accuracy: 0.5500 - val_loss: 1.2603\nEpoch 138/500\n\nEpoch 138: val_accuracy improved from 0.56400 to 0.57800, saving model to ckpt2.keras\n141/141 - 3s - 21ms/step - accuracy: 0.5736 - loss: 1.1371 - val_accuracy: 0.5780 - val_loss: 1.2366\nEpoch 139/500\n\nEpoch 139: val_accuracy did not improve from 0.57800\n141/141 - 3s - 21ms/step - accuracy: 0.5731 - loss: 1.1370 - val_accuracy: 0.5600 - val_loss: 1.2517\nEpoch 140/500\n\nEpoch 140: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5787 - loss: 1.1207 - val_accuracy: 0.5560 - val_loss: 1.2895\nEpoch 141/500\n\nEpoch 141: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5687 - loss: 1.1378 - val_accuracy: 0.5620 - val_loss: 1.2685\nEpoch 142/500\n\nEpoch 142: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5876 - loss: 1.1326 - val_accuracy: 0.5560 - val_loss: 1.2694\nEpoch 143/500\n\nEpoch 143: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5776 - loss: 1.1419 - val_accuracy: 0.5520 - val_loss: 1.2969\nEpoch 144/500\n\nEpoch 144: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5709 - loss: 1.1390 - val_accuracy: 0.5560 - val_loss: 1.3161\nEpoch 145/500\n\nEpoch 145: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5800 - loss: 1.1095 - val_accuracy: 0.5440 - val_loss: 1.2649\nEpoch 146/500\n\nEpoch 146: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5756 - loss: 1.1385 - val_accuracy: 0.5420 - val_loss: 1.3035\nEpoch 147/500\n\nEpoch 147: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5827 - loss: 1.1038 - val_accuracy: 0.5280 - val_loss: 1.2943\nEpoch 148/500\n\nEpoch 148: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5831 - loss: 1.1099 - val_accuracy: 0.5440 - val_loss: 1.2612\nEpoch 149/500\n\nEpoch 149: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5991 - loss: 1.1077 - val_accuracy: 0.5520 - val_loss: 1.3128\nEpoch 150/500\n\nEpoch 150: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5867 - loss: 1.0949 - val_accuracy: 0.5580 - val_loss: 1.3583\nEpoch 151/500\n\nEpoch 151: val_accuracy did not improve from 0.57800\n141/141 - 3s - 21ms/step - accuracy: 0.5844 - loss: 1.1044 - val_accuracy: 0.5720 - val_loss: 1.2522\nEpoch 152/500\n\nEpoch 152: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5867 - loss: 1.0952 - val_accuracy: 0.5520 - val_loss: 1.2694\nEpoch 153/500\n\nEpoch 153: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5916 - loss: 1.1009 - val_accuracy: 0.5320 - val_loss: 1.3104\nEpoch 154/500\n\nEpoch 154: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5864 - loss: 1.0934 - val_accuracy: 0.5620 - val_loss: 1.2752\nEpoch 155/500\n\nEpoch 155: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5956 - loss: 1.0822 - val_accuracy: 0.5400 - val_loss: 1.3531\nEpoch 156/500\n\nEpoch 156: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5880 - loss: 1.1004 - val_accuracy: 0.5620 - val_loss: 1.2688\nEpoch 157/500\n\nEpoch 157: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5856 - loss: 1.1174 - val_accuracy: 0.5560 - val_loss: 1.2298\nEpoch 158/500\n\nEpoch 158: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5942 - loss: 1.1123 - val_accuracy: 0.5520 - val_loss: 1.2764\nEpoch 159/500\n\nEpoch 159: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.6040 - loss: 1.0655 - val_accuracy: 0.5680 - val_loss: 1.3117\nEpoch 160/500\n\nEpoch 160: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5844 - loss: 1.0963 - val_accuracy: 0.5700 - val_loss: 1.2551\nEpoch 161/500\n\nEpoch 161: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5978 - loss: 1.0819 - val_accuracy: 0.5340 - val_loss: 1.2986\nEpoch 162/500\n\nEpoch 162: val_accuracy did not improve from 0.57800\n141/141 - 3s - 21ms/step - accuracy: 0.5967 - loss: 1.0732 - val_accuracy: 0.5300 - val_loss: 1.3294\nEpoch 163/500\n\nEpoch 163: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5973 - loss: 1.0874 - val_accuracy: 0.5480 - val_loss: 1.3169\nEpoch 164/500\n\nEpoch 164: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5980 - loss: 1.0942 - val_accuracy: 0.5560 - val_loss: 1.2737\nEpoch 165/500\n\nEpoch 165: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5904 - loss: 1.0762 - val_accuracy: 0.5420 - val_loss: 1.2828\nEpoch 166/500\n\nEpoch 166: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.6140 - loss: 1.0581 - val_accuracy: 0.5480 - val_loss: 1.2882\nEpoch 167/500\n\nEpoch 167: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5991 - loss: 1.0775 - val_accuracy: 0.5540 - val_loss: 1.2697\nEpoch 168/500\n\nEpoch 168: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.6100 - loss: 1.0605 - val_accuracy: 0.5540 - val_loss: 1.3095\nEpoch 169/500\n\nEpoch 169: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.5900 - loss: 1.0908 - val_accuracy: 0.5560 - val_loss: 1.2397\nEpoch 170/500\n\nEpoch 170: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.5987 - loss: 1.0769 - val_accuracy: 0.5320 - val_loss: 1.3057\nEpoch 171/500\n\nEpoch 171: val_accuracy did not improve from 0.57800\n141/141 - 3s - 19ms/step - accuracy: 0.6047 - loss: 1.0567 - val_accuracy: 0.5560 - val_loss: 1.2800\nEpoch 172/500\n\nEpoch 172: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.6093 - loss: 1.0432 - val_accuracy: 0.5620 - val_loss: 1.3183\nEpoch 173/500\n\nEpoch 173: val_accuracy did not improve from 0.57800\n141/141 - 3s - 20ms/step - accuracy: 0.6056 - loss: 1.0706 - val_accuracy: 0.5580 - val_loss: 1.2620\nEpoch 174/500\n\nEpoch 174: val_accuracy improved from 0.57800 to 0.59400, saving model to ckpt2.keras\n141/141 - 3s - 22ms/step - accuracy: 0.6151 - loss: 1.0509 - val_accuracy: 0.5940 - val_loss: 1.2517\nEpoch 175/500\n\nEpoch 175: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6107 - loss: 1.0437 - val_accuracy: 0.5660 - val_loss: 1.2869\nEpoch 176/500\n\nEpoch 176: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6033 - loss: 1.0496 - val_accuracy: 0.5620 - val_loss: 1.2823\nEpoch 177/500\n\nEpoch 177: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6044 - loss: 1.0429 - val_accuracy: 0.5380 - val_loss: 1.4189\nEpoch 178/500\n\nEpoch 178: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6109 - loss: 1.0564 - val_accuracy: 0.5520 - val_loss: 1.2600\nEpoch 179/500\n\nEpoch 179: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6040 - loss: 1.0725 - val_accuracy: 0.5460 - val_loss: 1.2938\nEpoch 180/500\n\nEpoch 180: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6182 - loss: 1.0269 - val_accuracy: 0.5560 - val_loss: 1.3472\nEpoch 181/500\n\nEpoch 181: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6102 - loss: 1.0541 - val_accuracy: 0.5480 - val_loss: 1.3180\nEpoch 182/500\n\nEpoch 182: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6193 - loss: 1.0437 - val_accuracy: 0.5700 - val_loss: 1.3094\nEpoch 183/500\n\nEpoch 183: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6116 - loss: 1.0405 - val_accuracy: 0.5780 - val_loss: 1.2587\nEpoch 184/500\n\nEpoch 184: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6176 - loss: 1.0324 - val_accuracy: 0.5600 - val_loss: 1.2985\nEpoch 185/500\n\nEpoch 185: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6136 - loss: 1.0528 - val_accuracy: 0.5480 - val_loss: 1.2767\nEpoch 186/500\n\nEpoch 186: val_accuracy did not improve from 0.59400\n141/141 - 5s - 35ms/step - accuracy: 0.6136 - loss: 1.0496 - val_accuracy: 0.5540 - val_loss: 1.3133\nEpoch 187/500\n\nEpoch 187: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6149 - loss: 1.0483 - val_accuracy: 0.5420 - val_loss: 1.2859\nEpoch 188/500\n\nEpoch 188: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6240 - loss: 1.0106 - val_accuracy: 0.5580 - val_loss: 1.2855\nEpoch 189/500\n\nEpoch 189: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6120 - loss: 1.0446 - val_accuracy: 0.5380 - val_loss: 1.2437\nEpoch 190/500\n\nEpoch 190: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6111 - loss: 1.0454 - val_accuracy: 0.5760 - val_loss: 1.2235\nEpoch 191/500\n\nEpoch 191: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6244 - loss: 1.0439 - val_accuracy: 0.5420 - val_loss: 1.2946\nEpoch 192/500\n\nEpoch 192: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6227 - loss: 1.0287 - val_accuracy: 0.5500 - val_loss: 1.2809\nEpoch 193/500\n\nEpoch 193: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6227 - loss: 1.0180 - val_accuracy: 0.5340 - val_loss: 1.3300\nEpoch 194/500\n\nEpoch 194: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6213 - loss: 1.0214 - val_accuracy: 0.5620 - val_loss: 1.2746\nEpoch 195/500\n\nEpoch 195: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6167 - loss: 1.0251 - val_accuracy: 0.5540 - val_loss: 1.3454\nEpoch 196/500\n\nEpoch 196: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6213 - loss: 1.0289 - val_accuracy: 0.5320 - val_loss: 1.2699\nEpoch 197/500\n\nEpoch 197: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6422 - loss: 0.9810 - val_accuracy: 0.5540 - val_loss: 1.2898\nEpoch 198/500\n\nEpoch 198: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6264 - loss: 1.0132 - val_accuracy: 0.5660 - val_loss: 1.2750\nEpoch 199/500\n\nEpoch 199: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6327 - loss: 1.0017 - val_accuracy: 0.5520 - val_loss: 1.3051\nEpoch 200/500\n\nEpoch 200: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6202 - loss: 1.0019 - val_accuracy: 0.5420 - val_loss: 1.3222\nEpoch 201/500\n\nEpoch 201: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6227 - loss: 1.0163 - val_accuracy: 0.5620 - val_loss: 1.3044\nEpoch 202/500\n\nEpoch 202: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6289 - loss: 0.9883 - val_accuracy: 0.5540 - val_loss: 1.3684\nEpoch 203/500\n\nEpoch 203: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6373 - loss: 0.9789 - val_accuracy: 0.5480 - val_loss: 1.2861\nEpoch 204/500\n\nEpoch 204: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6229 - loss: 1.0035 - val_accuracy: 0.5560 - val_loss: 1.3190\nEpoch 205/500\n\nEpoch 205: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6344 - loss: 0.9862 - val_accuracy: 0.5540 - val_loss: 1.3233\nEpoch 206/500\n\nEpoch 206: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6387 - loss: 1.0031 - val_accuracy: 0.5560 - val_loss: 1.3364\nEpoch 207/500\n\nEpoch 207: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6387 - loss: 0.9829 - val_accuracy: 0.5540 - val_loss: 1.3257\nEpoch 208/500\n\nEpoch 208: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6487 - loss: 0.9737 - val_accuracy: 0.5660 - val_loss: 1.3585\nEpoch 209/500\n\nEpoch 209: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6344 - loss: 0.9979 - val_accuracy: 0.5640 - val_loss: 1.2688\nEpoch 210/500\n\nEpoch 210: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6318 - loss: 0.9922 - val_accuracy: 0.5760 - val_loss: 1.3069\nEpoch 211/500\n\nEpoch 211: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6300 - loss: 0.9962 - val_accuracy: 0.5540 - val_loss: 1.2866\nEpoch 212/500\n\nEpoch 212: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6358 - loss: 0.9830 - val_accuracy: 0.5700 - val_loss: 1.2897\nEpoch 213/500\n\nEpoch 213: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6418 - loss: 0.9640 - val_accuracy: 0.5720 - val_loss: 1.2992\nEpoch 214/500\n\nEpoch 214: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6489 - loss: 0.9852 - val_accuracy: 0.5580 - val_loss: 1.2689\nEpoch 215/500\n\nEpoch 215: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6416 - loss: 0.9777 - val_accuracy: 0.5700 - val_loss: 1.2930\nEpoch 216/500\n\nEpoch 216: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6504 - loss: 0.9567 - val_accuracy: 0.5760 - val_loss: 1.3140\nEpoch 217/500\n\nEpoch 217: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6493 - loss: 0.9603 - val_accuracy: 0.5780 - val_loss: 1.3017\nEpoch 218/500\n\nEpoch 218: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6416 - loss: 0.9828 - val_accuracy: 0.5780 - val_loss: 1.3049\nEpoch 219/500\n\nEpoch 219: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6420 - loss: 0.9697 - val_accuracy: 0.5840 - val_loss: 1.3036\nEpoch 220/500\n\nEpoch 220: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6484 - loss: 0.9665 - val_accuracy: 0.5500 - val_loss: 1.3240\nEpoch 221/500\n\nEpoch 221: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6391 - loss: 0.9778 - val_accuracy: 0.5520 - val_loss: 1.2863\nEpoch 222/500\n\nEpoch 222: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6578 - loss: 0.9493 - val_accuracy: 0.5840 - val_loss: 1.2840\nEpoch 223/500\n\nEpoch 223: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6433 - loss: 0.9676 - val_accuracy: 0.5580 - val_loss: 1.3755\nEpoch 224/500\n\nEpoch 224: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6509 - loss: 0.9694 - val_accuracy: 0.5800 - val_loss: 1.2807\nEpoch 225/500\n\nEpoch 225: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6416 - loss: 0.9563 - val_accuracy: 0.5620 - val_loss: 1.2898\nEpoch 226/500\n\nEpoch 226: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6524 - loss: 0.9319 - val_accuracy: 0.5360 - val_loss: 1.3467\nEpoch 227/500\n\nEpoch 227: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6496 - loss: 0.9514 - val_accuracy: 0.5740 - val_loss: 1.2941\nEpoch 228/500\n\nEpoch 228: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6536 - loss: 0.9555 - val_accuracy: 0.5760 - val_loss: 1.3059\nEpoch 229/500\n\nEpoch 229: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6584 - loss: 0.9514 - val_accuracy: 0.5760 - val_loss: 1.2802\nEpoch 230/500\n\nEpoch 230: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6624 - loss: 0.9241 - val_accuracy: 0.5640 - val_loss: 1.2789\nEpoch 231/500\n\nEpoch 231: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6529 - loss: 0.9451 - val_accuracy: 0.5580 - val_loss: 1.2945\nEpoch 232/500\n\nEpoch 232: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6498 - loss: 0.9521 - val_accuracy: 0.5580 - val_loss: 1.2928\nEpoch 233/500\n\nEpoch 233: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6551 - loss: 0.9416 - val_accuracy: 0.5740 - val_loss: 1.3431\nEpoch 234/500\n\nEpoch 234: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6520 - loss: 0.9319 - val_accuracy: 0.5740 - val_loss: 1.3133\nEpoch 235/500\n\nEpoch 235: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6573 - loss: 0.9217 - val_accuracy: 0.5720 - val_loss: 1.3380\nEpoch 236/500\n\nEpoch 236: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6500 - loss: 0.9400 - val_accuracy: 0.5560 - val_loss: 1.3442\nEpoch 237/500\n\nEpoch 237: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6511 - loss: 0.9368 - val_accuracy: 0.5720 - val_loss: 1.2606\nEpoch 238/500\n\nEpoch 238: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6553 - loss: 0.9281 - val_accuracy: 0.5580 - val_loss: 1.3117\nEpoch 239/500\n\nEpoch 239: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6609 - loss: 0.9432 - val_accuracy: 0.5660 - val_loss: 1.3634\nEpoch 240/500\n\nEpoch 240: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6644 - loss: 0.9196 - val_accuracy: 0.5680 - val_loss: 1.3654\nEpoch 241/500\n\nEpoch 241: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6653 - loss: 0.9080 - val_accuracy: 0.5580 - val_loss: 1.3321\nEpoch 242/500\n\nEpoch 242: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6589 - loss: 0.9174 - val_accuracy: 0.5520 - val_loss: 1.3606\nEpoch 243/500\n\nEpoch 243: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6538 - loss: 0.9428 - val_accuracy: 0.5500 - val_loss: 1.3731\nEpoch 244/500\n\nEpoch 244: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6522 - loss: 0.9387 - val_accuracy: 0.5840 - val_loss: 1.3039\nEpoch 245/500\n\nEpoch 245: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6689 - loss: 0.9109 - val_accuracy: 0.5440 - val_loss: 1.3315\nEpoch 246/500\n\nEpoch 246: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6711 - loss: 0.9195 - val_accuracy: 0.5760 - val_loss: 1.3597\nEpoch 247/500\n\nEpoch 247: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6631 - loss: 0.9303 - val_accuracy: 0.5600 - val_loss: 1.3329\nEpoch 248/500\n\nEpoch 248: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6647 - loss: 0.9266 - val_accuracy: 0.5620 - val_loss: 1.3721\nEpoch 249/500\n\nEpoch 249: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6604 - loss: 0.9196 - val_accuracy: 0.5660 - val_loss: 1.3487\nEpoch 250/500\n\nEpoch 250: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6669 - loss: 0.9076 - val_accuracy: 0.5600 - val_loss: 1.3602\nEpoch 251/500\n\nEpoch 251: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6562 - loss: 0.9231 - val_accuracy: 0.5540 - val_loss: 1.3349\nEpoch 252/500\n\nEpoch 252: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6700 - loss: 0.9042 - val_accuracy: 0.5840 - val_loss: 1.3575\nEpoch 253/500\n\nEpoch 253: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6633 - loss: 0.9186 - val_accuracy: 0.5620 - val_loss: 1.3481\nEpoch 254/500\n\nEpoch 254: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.6676 - loss: 0.9261 - val_accuracy: 0.5620 - val_loss: 1.3379\nEpoch 255/500\n\nEpoch 255: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6704 - loss: 0.9006 - val_accuracy: 0.5700 - val_loss: 1.3364\nEpoch 256/500\n\nEpoch 256: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6676 - loss: 0.9060 - val_accuracy: 0.5620 - val_loss: 1.3558\nEpoch 257/500\n\nEpoch 257: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6651 - loss: 0.9161 - val_accuracy: 0.5820 - val_loss: 1.3477\nEpoch 258/500\n\nEpoch 258: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6824 - loss: 0.8821 - val_accuracy: 0.5760 - val_loss: 1.2845\nEpoch 259/500\n\nEpoch 259: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6700 - loss: 0.8890 - val_accuracy: 0.5640 - val_loss: 1.3191\nEpoch 260/500\n\nEpoch 260: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6698 - loss: 0.8762 - val_accuracy: 0.5700 - val_loss: 1.3958\nEpoch 261/500\n\nEpoch 261: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.6782 - loss: 0.8952 - val_accuracy: 0.5700 - val_loss: 1.3068\nEpoch 262/500\n\nEpoch 262: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.6862 - loss: 0.8814 - val_accuracy: 0.5480 - val_loss: 1.4208\nEpoch 263/500\n\nEpoch 263: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6811 - loss: 0.8722 - val_accuracy: 0.5720 - val_loss: 1.4258\nEpoch 264/500\n\nEpoch 264: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6793 - loss: 0.8932 - val_accuracy: 0.5700 - val_loss: 1.3899\nEpoch 265/500\n\nEpoch 265: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.6656 - loss: 0.8982 - val_accuracy: 0.5700 - val_loss: 1.3999\nEpoch 266/500\n\nEpoch 266: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6589 - loss: 0.9103 - val_accuracy: 0.5700 - val_loss: 1.4175\nEpoch 267/500\n\nEpoch 267: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6816 - loss: 0.8770 - val_accuracy: 0.5600 - val_loss: 1.3708\nEpoch 268/500\n\nEpoch 268: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6860 - loss: 0.8881 - val_accuracy: 0.5540 - val_loss: 1.4099\nEpoch 269/500\n\nEpoch 269: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6651 - loss: 0.9051 - val_accuracy: 0.5660 - val_loss: 1.3323\nEpoch 270/500\n\nEpoch 270: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6824 - loss: 0.8866 - val_accuracy: 0.5680 - val_loss: 1.3888\nEpoch 271/500\n\nEpoch 271: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6833 - loss: 0.8727 - val_accuracy: 0.5800 - val_loss: 1.3947\nEpoch 272/500\n\nEpoch 272: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6840 - loss: 0.8707 - val_accuracy: 0.5800 - val_loss: 1.3555\nEpoch 273/500\n\nEpoch 273: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6740 - loss: 0.8709 - val_accuracy: 0.5720 - val_loss: 1.4541\nEpoch 274/500\n\nEpoch 274: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6851 - loss: 0.8705 - val_accuracy: 0.5800 - val_loss: 1.4001\nEpoch 275/500\n\nEpoch 275: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6887 - loss: 0.8778 - val_accuracy: 0.5700 - val_loss: 1.3789\nEpoch 276/500\n\nEpoch 276: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6900 - loss: 0.8725 - val_accuracy: 0.5620 - val_loss: 1.3493\nEpoch 277/500\n\nEpoch 277: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.6922 - loss: 0.8591 - val_accuracy: 0.5880 - val_loss: 1.3777\nEpoch 278/500\n\nEpoch 278: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6831 - loss: 0.8830 - val_accuracy: 0.5660 - val_loss: 1.4792\nEpoch 279/500\n\nEpoch 279: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6924 - loss: 0.8591 - val_accuracy: 0.5840 - val_loss: 1.4580\nEpoch 280/500\n\nEpoch 280: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6911 - loss: 0.8525 - val_accuracy: 0.5740 - val_loss: 1.3900\nEpoch 281/500\n\nEpoch 281: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6949 - loss: 0.8618 - val_accuracy: 0.5740 - val_loss: 1.4330\nEpoch 282/500\n\nEpoch 282: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6847 - loss: 0.8498 - val_accuracy: 0.5920 - val_loss: 1.3316\nEpoch 283/500\n\nEpoch 283: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6936 - loss: 0.8468 - val_accuracy: 0.5580 - val_loss: 1.4312\nEpoch 284/500\n\nEpoch 284: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6924 - loss: 0.8403 - val_accuracy: 0.5600 - val_loss: 1.3975\nEpoch 285/500\n\nEpoch 285: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6951 - loss: 0.8453 - val_accuracy: 0.5800 - val_loss: 1.3592\nEpoch 286/500\n\nEpoch 286: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6911 - loss: 0.8611 - val_accuracy: 0.5680 - val_loss: 1.4367\nEpoch 287/500\n\nEpoch 287: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6809 - loss: 0.8531 - val_accuracy: 0.5880 - val_loss: 1.3779\nEpoch 288/500\n\nEpoch 288: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6962 - loss: 0.8431 - val_accuracy: 0.5920 - val_loss: 1.3537\nEpoch 289/500\n\nEpoch 289: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6922 - loss: 0.8395 - val_accuracy: 0.5820 - val_loss: 1.3899\nEpoch 290/500\n\nEpoch 290: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7042 - loss: 0.8283 - val_accuracy: 0.5600 - val_loss: 1.4243\nEpoch 291/500\n\nEpoch 291: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6987 - loss: 0.8358 - val_accuracy: 0.5600 - val_loss: 1.4899\nEpoch 292/500\n\nEpoch 292: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7067 - loss: 0.8106 - val_accuracy: 0.5620 - val_loss: 1.5795\nEpoch 293/500\n\nEpoch 293: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6933 - loss: 0.8319 - val_accuracy: 0.5500 - val_loss: 1.4704\nEpoch 294/500\n\nEpoch 294: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6902 - loss: 0.8551 - val_accuracy: 0.5680 - val_loss: 1.4855\nEpoch 295/500\n\nEpoch 295: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7058 - loss: 0.8174 - val_accuracy: 0.5660 - val_loss: 1.4492\nEpoch 296/500\n\nEpoch 296: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6927 - loss: 0.8477 - val_accuracy: 0.5620 - val_loss: 1.3785\nEpoch 297/500\n\nEpoch 297: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7082 - loss: 0.8046 - val_accuracy: 0.5600 - val_loss: 1.5129\nEpoch 298/500\n\nEpoch 298: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7047 - loss: 0.8171 - val_accuracy: 0.5420 - val_loss: 1.5268\nEpoch 299/500\n\nEpoch 299: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6996 - loss: 0.8212 - val_accuracy: 0.5660 - val_loss: 1.3935\nEpoch 300/500\n\nEpoch 300: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7009 - loss: 0.8219 - val_accuracy: 0.5720 - val_loss: 1.4664\nEpoch 301/500\n\nEpoch 301: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6982 - loss: 0.8563 - val_accuracy: 0.5840 - val_loss: 1.4127\nEpoch 302/500\n\nEpoch 302: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.6993 - loss: 0.8149 - val_accuracy: 0.5900 - val_loss: 1.4567\nEpoch 303/500\n\nEpoch 303: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7022 - loss: 0.8136 - val_accuracy: 0.5520 - val_loss: 1.5146\nEpoch 304/500\n\nEpoch 304: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7031 - loss: 0.8317 - val_accuracy: 0.5660 - val_loss: 1.4296\nEpoch 305/500\n\nEpoch 305: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.6969 - loss: 0.8468 - val_accuracy: 0.5840 - val_loss: 1.3875\nEpoch 306/500\n\nEpoch 306: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7164 - loss: 0.8092 - val_accuracy: 0.5780 - val_loss: 1.4675\nEpoch 307/500\n\nEpoch 307: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7020 - loss: 0.8280 - val_accuracy: 0.5500 - val_loss: 1.5292\nEpoch 308/500\n\nEpoch 308: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7060 - loss: 0.8076 - val_accuracy: 0.5920 - val_loss: 1.4191\nEpoch 309/500\n\nEpoch 309: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7062 - loss: 0.8012 - val_accuracy: 0.5880 - val_loss: 1.5297\nEpoch 310/500\n\nEpoch 310: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7158 - loss: 0.7980 - val_accuracy: 0.5720 - val_loss: 1.6002\nEpoch 311/500\n\nEpoch 311: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7129 - loss: 0.8097 - val_accuracy: 0.5820 - val_loss: 1.5119\nEpoch 312/500\n\nEpoch 312: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7138 - loss: 0.7839 - val_accuracy: 0.5800 - val_loss: 1.4802\nEpoch 313/500\n\nEpoch 313: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7060 - loss: 0.8000 - val_accuracy: 0.5760 - val_loss: 1.4912\nEpoch 314/500\n\nEpoch 314: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7162 - loss: 0.7985 - val_accuracy: 0.5640 - val_loss: 1.5029\nEpoch 315/500\n\nEpoch 315: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7144 - loss: 0.8093 - val_accuracy: 0.5540 - val_loss: 1.5523\nEpoch 316/500\n\nEpoch 316: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7098 - loss: 0.8061 - val_accuracy: 0.5620 - val_loss: 1.5411\nEpoch 317/500\n\nEpoch 317: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7151 - loss: 0.8033 - val_accuracy: 0.5860 - val_loss: 1.5223\nEpoch 318/500\n\nEpoch 318: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7251 - loss: 0.7835 - val_accuracy: 0.5720 - val_loss: 1.5290\nEpoch 319/500\n\nEpoch 319: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7253 - loss: 0.7739 - val_accuracy: 0.5520 - val_loss: 1.7244\nEpoch 320/500\n\nEpoch 320: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7176 - loss: 0.7847 - val_accuracy: 0.5540 - val_loss: 1.6150\nEpoch 321/500\n\nEpoch 321: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7244 - loss: 0.7708 - val_accuracy: 0.5500 - val_loss: 1.5811\nEpoch 322/500\n\nEpoch 322: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7227 - loss: 0.7835 - val_accuracy: 0.5560 - val_loss: 1.6351\nEpoch 323/500\n\nEpoch 323: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7127 - loss: 0.7808 - val_accuracy: 0.5520 - val_loss: 1.6125\nEpoch 324/500\n\nEpoch 324: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7196 - loss: 0.7748 - val_accuracy: 0.5560 - val_loss: 1.6511\nEpoch 325/500\n\nEpoch 325: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7251 - loss: 0.7978 - val_accuracy: 0.5720 - val_loss: 1.5136\nEpoch 326/500\n\nEpoch 326: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7227 - loss: 0.7789 - val_accuracy: 0.5640 - val_loss: 1.5357\nEpoch 327/500\n\nEpoch 327: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7218 - loss: 0.7784 - val_accuracy: 0.5540 - val_loss: 1.7206\nEpoch 328/500\n\nEpoch 328: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7196 - loss: 0.7738 - val_accuracy: 0.5580 - val_loss: 1.5862\nEpoch 329/500\n\nEpoch 329: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7318 - loss: 0.7565 - val_accuracy: 0.5540 - val_loss: 1.7043\nEpoch 330/500\n\nEpoch 330: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7258 - loss: 0.7711 - val_accuracy: 0.5560 - val_loss: 1.5307\nEpoch 331/500\n\nEpoch 331: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7193 - loss: 0.7893 - val_accuracy: 0.5540 - val_loss: 1.5416\nEpoch 332/500\n\nEpoch 332: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7209 - loss: 0.7794 - val_accuracy: 0.5440 - val_loss: 1.4911\nEpoch 333/500\n\nEpoch 333: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7209 - loss: 0.7796 - val_accuracy: 0.5620 - val_loss: 1.5270\nEpoch 334/500\n\nEpoch 334: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7238 - loss: 0.7673 - val_accuracy: 0.5420 - val_loss: 1.7382\nEpoch 335/500\n\nEpoch 335: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7287 - loss: 0.7583 - val_accuracy: 0.5580 - val_loss: 1.5707\nEpoch 336/500\n\nEpoch 336: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7287 - loss: 0.7651 - val_accuracy: 0.5660 - val_loss: 1.6995\nEpoch 337/500\n\nEpoch 337: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7162 - loss: 0.7716 - val_accuracy: 0.5760 - val_loss: 1.5830\nEpoch 338/500\n\nEpoch 338: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7378 - loss: 0.7550 - val_accuracy: 0.5620 - val_loss: 1.5610\nEpoch 339/500\n\nEpoch 339: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7251 - loss: 0.7610 - val_accuracy: 0.5540 - val_loss: 1.7795\nEpoch 340/500\n\nEpoch 340: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7202 - loss: 0.7718 - val_accuracy: 0.5440 - val_loss: 1.5798\nEpoch 341/500\n\nEpoch 341: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7233 - loss: 0.7651 - val_accuracy: 0.5500 - val_loss: 1.5637\nEpoch 342/500\n\nEpoch 342: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7322 - loss: 0.7644 - val_accuracy: 0.5700 - val_loss: 1.6268\nEpoch 343/500\n\nEpoch 343: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7309 - loss: 0.7527 - val_accuracy: 0.5680 - val_loss: 1.5972\nEpoch 344/500\n\nEpoch 344: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7362 - loss: 0.7563 - val_accuracy: 0.5860 - val_loss: 1.6342\nEpoch 345/500\n\nEpoch 345: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7338 - loss: 0.7572 - val_accuracy: 0.5540 - val_loss: 1.7094\nEpoch 346/500\n\nEpoch 346: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7211 - loss: 0.7637 - val_accuracy: 0.5700 - val_loss: 1.6220\nEpoch 347/500\n\nEpoch 347: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7313 - loss: 0.7530 - val_accuracy: 0.5440 - val_loss: 1.6706\nEpoch 348/500\n\nEpoch 348: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7402 - loss: 0.7310 - val_accuracy: 0.5720 - val_loss: 1.6389\nEpoch 349/500\n\nEpoch 349: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7311 - loss: 0.7502 - val_accuracy: 0.5580 - val_loss: 1.6932\nEpoch 350/500\n\nEpoch 350: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7338 - loss: 0.7460 - val_accuracy: 0.5480 - val_loss: 1.6363\nEpoch 351/500\n\nEpoch 351: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7393 - loss: 0.7313 - val_accuracy: 0.5580 - val_loss: 1.6412\nEpoch 352/500\n\nEpoch 352: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7324 - loss: 0.7416 - val_accuracy: 0.5500 - val_loss: 1.6150\nEpoch 353/500\n\nEpoch 353: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7542 - loss: 0.7127 - val_accuracy: 0.5740 - val_loss: 1.5889\nEpoch 354/500\n\nEpoch 354: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7482 - loss: 0.7228 - val_accuracy: 0.5660 - val_loss: 1.6174\nEpoch 355/500\n\nEpoch 355: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7376 - loss: 0.7361 - val_accuracy: 0.5620 - val_loss: 1.6494\nEpoch 356/500\n\nEpoch 356: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7364 - loss: 0.7394 - val_accuracy: 0.5360 - val_loss: 1.7463\nEpoch 357/500\n\nEpoch 357: val_accuracy did not improve from 0.59400\n141/141 - 3s - 19ms/step - accuracy: 0.7420 - loss: 0.7410 - val_accuracy: 0.5580 - val_loss: 1.6780\nEpoch 358/500\n\nEpoch 358: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7404 - loss: 0.7450 - val_accuracy: 0.5880 - val_loss: 1.5926\nEpoch 359/500\n\nEpoch 359: val_accuracy did not improve from 0.59400\n141/141 - 5s - 36ms/step - accuracy: 0.7478 - loss: 0.7251 - val_accuracy: 0.5660 - val_loss: 1.6514\nEpoch 360/500\n\nEpoch 360: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7429 - loss: 0.7363 - val_accuracy: 0.5500 - val_loss: 1.6591\nEpoch 361/500\n\nEpoch 361: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7453 - loss: 0.7194 - val_accuracy: 0.5500 - val_loss: 1.7654\nEpoch 362/500\n\nEpoch 362: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7382 - loss: 0.7351 - val_accuracy: 0.5720 - val_loss: 1.6933\nEpoch 363/500\n\nEpoch 363: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7333 - loss: 0.7276 - val_accuracy: 0.5780 - val_loss: 1.6560\nEpoch 364/500\n\nEpoch 364: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7376 - loss: 0.7340 - val_accuracy: 0.5600 - val_loss: 1.6332\nEpoch 365/500\n\nEpoch 365: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7373 - loss: 0.7328 - val_accuracy: 0.5780 - val_loss: 1.7326\nEpoch 366/500\n\nEpoch 366: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7536 - loss: 0.7117 - val_accuracy: 0.5580 - val_loss: 1.6346\nEpoch 367/500\n\nEpoch 367: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7544 - loss: 0.7222 - val_accuracy: 0.5620 - val_loss: 1.6224\nEpoch 368/500\n\nEpoch 368: val_accuracy did not improve from 0.59400\n141/141 - 3s - 22ms/step - accuracy: 0.7438 - loss: 0.7140 - val_accuracy: 0.5640 - val_loss: 1.6858\nEpoch 369/500\n\nEpoch 369: val_accuracy did not improve from 0.59400\n141/141 - 3s - 22ms/step - accuracy: 0.7462 - loss: 0.7313 - val_accuracy: 0.5440 - val_loss: 1.6926\nEpoch 370/500\n\nEpoch 370: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7498 - loss: 0.7031 - val_accuracy: 0.5600 - val_loss: 1.7155\nEpoch 371/500\n\nEpoch 371: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7353 - loss: 0.7453 - val_accuracy: 0.5740 - val_loss: 1.6089\nEpoch 372/500\n\nEpoch 372: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7487 - loss: 0.7076 - val_accuracy: 0.5680 - val_loss: 1.6660\nEpoch 373/500\n\nEpoch 373: val_accuracy did not improve from 0.59400\n141/141 - 5s - 36ms/step - accuracy: 0.7518 - loss: 0.7068 - val_accuracy: 0.5700 - val_loss: 1.7859\nEpoch 374/500\n\nEpoch 374: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7338 - loss: 0.7360 - val_accuracy: 0.5880 - val_loss: 1.6032\nEpoch 375/500\n\nEpoch 375: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7409 - loss: 0.7253 - val_accuracy: 0.5600 - val_loss: 1.7994\nEpoch 376/500\n\nEpoch 376: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7560 - loss: 0.6947 - val_accuracy: 0.5700 - val_loss: 1.7729\nEpoch 377/500\n\nEpoch 377: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7511 - loss: 0.7058 - val_accuracy: 0.5680 - val_loss: 1.7202\nEpoch 378/500\n\nEpoch 378: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7460 - loss: 0.7068 - val_accuracy: 0.5580 - val_loss: 1.7611\nEpoch 379/500\n\nEpoch 379: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7484 - loss: 0.6942 - val_accuracy: 0.5680 - val_loss: 1.7705\nEpoch 380/500\n\nEpoch 380: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7516 - loss: 0.7012 - val_accuracy: 0.5540 - val_loss: 1.6896\nEpoch 381/500\n\nEpoch 381: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7620 - loss: 0.6874 - val_accuracy: 0.5380 - val_loss: 1.8139\nEpoch 382/500\n\nEpoch 382: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7618 - loss: 0.7010 - val_accuracy: 0.5500 - val_loss: 1.7328\nEpoch 383/500\n\nEpoch 383: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7564 - loss: 0.6967 - val_accuracy: 0.5700 - val_loss: 1.7823\nEpoch 384/500\n\nEpoch 384: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7558 - loss: 0.6747 - val_accuracy: 0.5760 - val_loss: 1.8151\nEpoch 385/500\n\nEpoch 385: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7536 - loss: 0.7080 - val_accuracy: 0.5620 - val_loss: 1.6243\nEpoch 386/500\n\nEpoch 386: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7540 - loss: 0.6940 - val_accuracy: 0.5460 - val_loss: 1.7505\nEpoch 387/500\n\nEpoch 387: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7702 - loss: 0.6662 - val_accuracy: 0.5580 - val_loss: 1.7194\nEpoch 388/500\n\nEpoch 388: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7536 - loss: 0.7081 - val_accuracy: 0.5520 - val_loss: 1.6828\nEpoch 389/500\n\nEpoch 389: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7547 - loss: 0.6907 - val_accuracy: 0.5620 - val_loss: 1.7492\nEpoch 390/500\n\nEpoch 390: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7529 - loss: 0.7043 - val_accuracy: 0.5600 - val_loss: 1.5989\nEpoch 391/500\n\nEpoch 391: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7580 - loss: 0.6929 - val_accuracy: 0.5780 - val_loss: 1.6601\nEpoch 392/500\n\nEpoch 392: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7573 - loss: 0.6720 - val_accuracy: 0.5740 - val_loss: 1.7567\nEpoch 393/500\n\nEpoch 393: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7571 - loss: 0.6781 - val_accuracy: 0.5900 - val_loss: 1.7360\nEpoch 394/500\n\nEpoch 394: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7622 - loss: 0.6734 - val_accuracy: 0.5580 - val_loss: 1.8098\nEpoch 395/500\n\nEpoch 395: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7624 - loss: 0.6815 - val_accuracy: 0.5720 - val_loss: 1.7159\nEpoch 396/500\n\nEpoch 396: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7578 - loss: 0.6950 - val_accuracy: 0.5520 - val_loss: 1.7616\nEpoch 397/500\n\nEpoch 397: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7618 - loss: 0.6679 - val_accuracy: 0.5660 - val_loss: 1.6898\nEpoch 398/500\n\nEpoch 398: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7576 - loss: 0.6806 - val_accuracy: 0.5560 - val_loss: 1.7603\nEpoch 399/500\n\nEpoch 399: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7669 - loss: 0.6572 - val_accuracy: 0.5600 - val_loss: 1.7924\nEpoch 400/500\n\nEpoch 400: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7684 - loss: 0.6550 - val_accuracy: 0.5720 - val_loss: 1.7489\nEpoch 401/500\n\nEpoch 401: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7620 - loss: 0.6765 - val_accuracy: 0.5560 - val_loss: 1.8489\nEpoch 402/500\n\nEpoch 402: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7649 - loss: 0.6818 - val_accuracy: 0.5340 - val_loss: 1.7077\nEpoch 403/500\n\nEpoch 403: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7693 - loss: 0.6514 - val_accuracy: 0.5620 - val_loss: 1.8539\nEpoch 404/500\n\nEpoch 404: val_accuracy did not improve from 0.59400\n141/141 - 3s - 20ms/step - accuracy: 0.7620 - loss: 0.6633 - val_accuracy: 0.5560 - val_loss: 1.8994\nEpoch 405/500\n\nEpoch 405: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7607 - loss: 0.6819 - val_accuracy: 0.5680 - val_loss: 1.6635\nEpoch 406/500\n\nEpoch 406: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7698 - loss: 0.6597 - val_accuracy: 0.5460 - val_loss: 1.7985\nEpoch 407/500\n\nEpoch 407: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7627 - loss: 0.6701 - val_accuracy: 0.5420 - val_loss: 1.7028\nEpoch 408/500\n\nEpoch 408: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7700 - loss: 0.6493 - val_accuracy: 0.5460 - val_loss: 1.8946\nEpoch 409/500\n\nEpoch 409: val_accuracy did not improve from 0.59400\n141/141 - 3s - 21ms/step - accuracy: 0.7618 - loss: 0.6752 - val_accuracy: 0.5420 - val_loss: 1.8442\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e4a76dbfdc0>"},"metadata":{}}]},{"cell_type":"code","source":"#evalute the checkpoint\nmodel.load_weights('ckpt2.keras')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:25:12.559647Z","iopub.execute_input":"2024-09-16T17:25:12.560622Z","iopub.status.idle":"2024-09-16T17:25:12.921202Z","shell.execute_reply.started":"2024-09-16T17:25:12.560575Z","shell.execute_reply":"2024-09-16T17:25:12.920243Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.4f}')\n\n# Predict emotions on the test set\npredictions = model.predict(X_test)\npredicted_labels = np.argmax(predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:25:14.686947Z","iopub.execute_input":"2024-09-16T17:25:14.687387Z","iopub.status.idle":"2024-09-16T17:25:16.858405Z","shell.execute_reply.started":"2024-09-16T17:25:14.687346Z","shell.execute_reply":"2024-09-16T17:25:16.856815Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5796 - loss: 1.2472\nTest Accuracy: 0.5940\n\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step \n","output_type":"stream"}]},{"cell_type":"code","source":"test=test.apply(pd.to_numeric)\ntest=test.values\nimage_size = 48\ntest = test.reshape(-1, image_size, image_size, 1)  # Reshape to 48x48 images with 1 channel (grayscale)\n# Normalize pixel values to [0, 1]\ntest = test / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:25:16.860768Z","iopub.execute_input":"2024-09-16T17:25:16.861274Z","iopub.status.idle":"2024-09-16T17:25:24.157579Z","shell.execute_reply.started":"2024-09-16T17:25:16.861223Z","shell.execute_reply":"2024-09-16T17:25:24.156491Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Predict emotions on the test set\npredictions = model.predict(test)\npredicted_labels = np.argmax(predictions, axis=1)  # Get the class with the highest probability\n\n# Assuming you have a corresponding 'id' column in your test data\n# We need to extract the 'id' from the original dataset corresponding to X_test\n# First, make sure to retrieve the correct indices from the original data split\n# Convert predictions and ids to DataFrame\nsubmission_df = pd.DataFrame({\n    'id': id,\n    'emotion': predicted_labels\n})\n\n# Save predictions to CSV file\nsubmission_df.to_csv('submission1.csv', index=False)\n\nprint(\"Submission saved to 'submission1.csv'.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:25:24.158845Z","iopub.execute_input":"2024-09-16T17:25:24.159205Z","iopub.status.idle":"2024-09-16T17:25:25.452614Z","shell.execute_reply.started":"2024-09-16T17:25:24.159169Z","shell.execute_reply":"2024-09-16T17:25:25.451467Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\nSubmission saved to 'submission1.csv'.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}